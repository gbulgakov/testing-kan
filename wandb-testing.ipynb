{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install delu\n!pip install ucimlrepo\n!pip install gdown\n!pip install rtdl_num_embeddings\n!pip install wandb\n# !pip install optuna\n!git clone https://github.com/gbulgakov/testing-kan.git\n","metadata":{"collapsed":true,"execution":{"iopub.status.busy":"2025-04-14T11:07:48.739188Z","iopub.execute_input":"2025-04-14T11:07:48.739430Z","iopub.status.idle":"2025-04-14T11:09:25.634720Z","shell.execute_reply.started":"2025-04-14T11:07:48.739411Z","shell.execute_reply":"2025-04-14T11:09:25.633770Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting delu\n  Downloading delu-0.0.26-py3-none-any.whl.metadata (805 bytes)\nRequirement already satisfied: numpy<3,>=1.21 in /usr/local/lib/python3.11/dist-packages (from delu) (1.26.4)\nRequirement already satisfied: torch<3,>=1.9 in /usr/local/lib/python3.11/dist-packages (from delu) (2.5.1+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21->delu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21->delu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21->delu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21->delu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21->delu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21->delu) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=1.9->delu)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=1.9->delu)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=1.9->delu)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=1.9->delu)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=1.9->delu)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=1.9->delu)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=1.9->delu)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->delu) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=1.9->delu) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=1.9->delu) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.21->delu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.21->delu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.21->delu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.21->delu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.21->delu) (2024.2.0)\nDownloading delu-0.0.26-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, delu\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed delu-0.0.26 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nCollecting ucimlrepo\n  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.3)\nRequirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.1.31)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nDownloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\nInstalling collected packages: ucimlrepo\nSuccessfully installed ucimlrepo-0.0.7\nRequirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\nCollecting rtdl_num_embeddings\n  Downloading rtdl_num_embeddings-0.0.12-py3-none-any.whl.metadata (903 bytes)\nRequirement already satisfied: torch<3,>=1.12 in /usr/local/lib/python3.11/dist-packages (from rtdl_num_embeddings) (2.5.1+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=1.12->rtdl_num_embeddings) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=1.12->rtdl_num_embeddings) (3.0.2)\nDownloading rtdl_num_embeddings-0.0.12-py3-none-any.whl (13 kB)\nInstalling collected packages: rtdl_num_embeddings\nSuccessfully installed rtdl_num_embeddings-0.0.12\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nCloning into 'testing-kan'...\nremote: Enumerating objects: 168, done.\u001b[K\nremote: Counting objects: 100% (7/7), done.\u001b[K\nremote: Compressing objects: 100% (7/7), done.\u001b[K\nremote: Total 168 (delta 1), reused 0 (delta 0), pack-reused 161 (from 1)\u001b[K\nReceiving objects: 100% (168/168), 91.50 KiB | 13.07 MiB/s, done.\nResolving deltas: 100% (81/81), done.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport optuna\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ucimlrepo import fetch_ucirepo\nfrom typing import Literal, Optional\nfrom torch import Tensor\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nimport rtdl_num_embeddings\nimport delu\nfrom IPython.display import FileLink\nfrom tqdm import tqdm\nimport wandb\nwandb.login(key='936d887040f82c8da3d87f5207c4178259c7b922')\n\n\n# наши импорты\nsys.path.append('/kaggle/working/testing-kan/optimizers')\nsys.path.append('/kaggle/working/testing-kan')\nfrom ademamix import AdEMAMix\nfrom mars import MARS\nfrom muon import Muon\nfrom efficient_kan import KAN\nfrom utils import utils","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:09:25.636416Z","iopub.execute_input":"2025-04-14T11:09:25.636754Z","iopub.status.idle":"2025-04-14T11:09:37.241447Z","shell.execute_reply.started":"2025-04-14T11:09:25.636732Z","shell.execute_reply":"2025-04-14T11:09:37.240922Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgbulgakov\u001b[0m (\u001b[33mgeorgy-bulgakov\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# !gdown 1xvRa_-OEeG6xNRYE5V5iAfTwyWM1NiLl # otto\n# !gdown 1tYyhbHdYs_8I9jvXznMoeUAfBzwitaax # house\n# !gdown 1hy1dOAL2SE-XZSuMcjLcVgml2CoYkF9q # higgs-small\n# !gdown 1hr076cK9QFxH6YZRg5V4av-H7IAve59r # gesture\n# !gdown 1ZNScy5fgqtgudT6MZ4EjLt1nwdqirtmX # fb-comments\n!gdown 1s0w7gnhiwBCkF49Wdi_cUDpUtXlz2_6q # eye\n# !gdown 1T04iP04UGVo95Om84ww1Ed8AFNziOaeY # covtype\n!gdown 1GOkNlinj4zHVSNKbqjN1rR4cvsAf2IgR # churn\n# !gdown 11B-l4EasJkclK_Q-RBcxYfGJLSvz-v5c # california\n# !gdown 1p8uqDPMfRlFIc69m7iikS6wGkA6JGj1H # adult","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:07:37.045979Z","iopub.execute_input":"2025-04-14T11:07:37.046451Z","iopub.status.idle":"2025-04-14T11:07:48.737715Z","shell.execute_reply.started":"2025-04-14T11:07:37.046430Z","shell.execute_reply":"2025-04-14T11:07:48.736781Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1s0w7gnhiwBCkF49Wdi_cUDpUtXlz2_6q\nTo: /kaggle/working/eye.zip\n100%|████████████████████████████████████████| 534k/534k [00:00<00:00, 1.62MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1GOkNlinj4zHVSNKbqjN1rR4cvsAf2IgR\nTo: /kaggle/working/churn.zip\n100%|████████████████████████████████████████| 453k/453k [00:00<00:00, 1.46MB/s]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Я вынес в файл ``utils`` функции ``count_parameters``,``load_dataset``, ``seed_everything``,  ``write_results``. \nМодели не меняем.","metadata":{}},{"cell_type":"code","source":"DATASETS = ['adult', 'california', 'churn', 'covtype', 'eye', 'fb-comments',\n           'gesture', 'higgs-small', 'house', 'microsoft', 'otto', 'santander']\n\n\nBATCH_SIZES = {'gesture' : 128, 'churn' : 128, 'california' : 256, 'house' : 256, 'adult' : 256, 'otto' : 512, \n               'higgs-small' : 512, 'fb-comments' : 512, 'santander' : 1024, 'covtype' : 1024, 'microsoft' : 1024, 'eye': 128}\n\nREGRESSION = ['house', 'fb-comments', 'microsoft', 'california']\nMULTICLASS = ['covtype', 'eye', 'gesture', 'otto']\nBINCLASS = ['adult', 'churn', 'higgs-small', 'santander']\n\nOPTIMIZERS = {'adamw' : torch.optim.AdamW,\n              'mars' : MARS,\n              'ademamix' : AdEMAMix,\n              'muon' : Muon}","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:09:41.573138Z","iopub.execute_input":"2025-04-14T11:09:41.573412Z","iopub.status.idle":"2025-04-14T11:09:41.578333Z","shell.execute_reply.started":"2025-04-14T11:09:41.573389Z","shell.execute_reply":"2025-04-14T11:09:41.577674Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"Модели не меняем.","metadata":{}},{"cell_type":"code","source":"class MLP(nn.Sequential):\n    def __init__(self, layers, dropout):\n        super(MLP, self).__init__()\n        \n        total_layers = []\n        for n_in, n_out in zip(layers[:-2], layers[1:-1]):\n            total_layers.append(nn.Linear(n_in, n_out))\n            total_layers.append(nn.SiLU(inplace=False))\n            total_layers.append(nn.Dropout(dropout, inplace=False))\n        total_layers.append(nn.Linear(layers[-2], layers[-1])) # выходной слой\n\n        self.classifier = nn.Sequential(*total_layers)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:09:43.046825Z","iopub.execute_input":"2025-04-14T11:09:43.047133Z","iopub.status.idle":"2025-04-14T11:09:43.052344Z","shell.execute_reply.started":"2025-04-14T11:09:43.047110Z","shell.execute_reply":"2025-04-14T11:09:43.051584Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class ModelWithEmbedding(nn.Module):\n    def __init__(\n        self,\n        n_cont_features,\n        d_embedding,\n        emb_name,\n        backbone_model,\n        bins, sigma=None # словарь всех необязательных параметров, например sigma, bins\n    ) -> None:\n        super().__init__()\n        self.d_embedding = d_embedding\n        self.emb_name = emb_name\n        \n        if emb_name == 'periodic':\n            self.cont_embeddings = rtdl_num_embeddings.PeriodicEmbeddings(\n                n_cont_features, d_embedding, frequency_init_scale=sigma, lite=True\n            )\n            \n        if emb_name == 'piecewiselinearq' or emb_name == 'piecewiselineart' or emb_name == 'PLE-Q' or emb_name == 'PLE-T':\n            self.cont_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n                d_embedding=d_embedding, activation=False, version='B', bins=bins\n            )\n\n        self.backbone = backbone_model\n    \n    def forward(\n        self,\n        x_num : Tensor,\n        x_cat : Optional[Tensor] = None\n    ) -> Tensor:\n        x = []\n        # Step 1. Embed the continuous features.\n        # Flattening is needed for MLP-like models.\n        if self.emb_name != 'none':\n              x_num = self.cont_embeddings(x_num)\n        x.append(x_num.flatten(1))\n        \n        #categorical features do not need embeddings\n        if x_cat is not None:\n            x.append(x_cat.flatten(1))\n        \n        x = torch.column_stack(x)\n        return self.backbone(x)","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:09:44.046545Z","iopub.execute_input":"2025-04-14T11:09:44.046835Z","iopub.status.idle":"2025-04-14T11:09:44.053444Z","shell.execute_reply.started":"2025-04-14T11:09:44.046806Z","shell.execute_reply":"2025-04-14T11:09:44.052823Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Заготовим функцию, которая создает конфигурацию тюнинга.\n\nПодбор параметров не меняем, тюним архитектуру сеток, ``lr``, ``weight_decay``.","metadata":{}},{"cell_type":"code","source":"def get_sweep_config(model_name, emb_name, task_type, sweep_name=''):\n    metric = {}\n    if task_type == 'regression':\n        metric = {'name' : 'val_loss', 'goal' : 'minimize'}\n    else:\n        metric = {'name' : 'val_acc', 'goal' : 'maximize'}\n    \n    max_log_width = (7 if model_name == 'kan' else 11)\n    params = {\n        'lr' : {\n            'distribution' : 'log_uniform_values',\n            'min' : 1e-5,\n            'max' : 1e-2\n        },\n        'weight_decay' : {\n            'distribution' : 'log_uniform_values',\n            'min' : 1e-6,\n            'max' : 1e-3\n        },\n        'hidden_layers' : { # скрытые слои\n            'values' : [1, 2, 3, 4]\n        },\n        'layer_width' : {\n            'values' : [2 ** i for i in range(max_log_width)]\n        },\n        'grid_size' : {\n            'values' : [i for i in range(3, 30, 2)]\n        }\n    }\n    if model_name == 'mlp':\n        params.update({\n            'use_dropout' : {'values' : [True, False]},\n            'dropout' : {'values' : [i / 100 for i in range(0, 55, 5)]}\n        })\n    if emb_name != 'none':\n        params.update({\n            'd_embedding' : {'values' : [2 ** i for i in range(1, 8)]}\n        })\n    if emb_name == 'periodic':\n        params.update({\n            'sigma' : {\n                'distribution' : 'uniform',\n                'min' : 0.01,\n                'max' : 100\n            }\n        })\n    \n    config = {\n        'method' : 'random',\n        'metric' : metric,\n        'parameters' : params,\n        'name' : sweep_name\n    }\n    return config","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:09:45.139962Z","iopub.execute_input":"2025-04-14T11:09:45.140242Z","iopub.status.idle":"2025-04-14T11:09:45.147335Z","shell.execute_reply.started":"2025-04-14T11:09:45.140221Z","shell.execute_reply":"2025-04-14T11:09:45.146615Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def get_test_config(model_name, emb_name, task_type, sweep_name=''):\n    metric = {} # чисто технический параметр\n    if task_type == 'regression':\n        metric = {'name' : 'val_loss', 'goal' : 'minimize'}\n    else:\n        metric = {'name' : 'val_acc', 'goal' : 'maximize'}\n    params = {\n        'seed' : {\n            'values' : [i for i in range(10)]\n        }\n    }\n    config = {\n        'method' : 'grid',\n        'metric' : metric,\n        'parameters' : params,\n        'name' : sweep_name\n    }\n    return config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:09:45.288453Z","iopub.execute_input":"2025-04-14T11:09:45.288721Z","iopub.status.idle":"2025-04-14T11:09:45.293647Z","shell.execute_reply.started":"2025-04-14T11:09:45.288700Z","shell.execute_reply":"2025-04-14T11:09:45.292576Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"В ``train`` теперь передаем целиком ``optimizer``.  Кроме того, разнесем обучение по эпохам.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom torch.optim import AdamW, Adam\nfrom torch.optim.lr_scheduler import ExponentialLR, StepLR, CosineAnnealingLR\nfrom torch.nn import MSELoss\nimport torch.nn as nn\nimport time\n\n\ndef apply_model(batch: dict[str, Tensor], model) -> Tensor:\n    return model(batch['X_num'], batch.get('X_cat')).squeeze(-1)\n\ndef train_epoch(model, device, dataset, loss_fn, optimizer, scheduler):\n\n    dataset_name = dataset['info']['id'].split('--')[0]\n    task_type = dataset['info']['task_type']\n    batch_size = BATCH_SIZES[dataset_name]\n\n    model.to(device)\n    model.train()\n    train_loss = 0.0\n    pred = []\n    gt = [] # настоящие таргеты\n    start_time = time.time()\n\n    for data in delu.iter_batches(dataset['train'], shuffle=True, batch_size=batch_size):\n        for key, tensor in data.items():\n            data[key] = tensor.to(device)\n        # обучение\n        optimizer.zero_grad()\n        output = apply_model(data, model)\n        if task_type == 'multiclass':\n            data['y'] = data['y'].long()\n        loss_value = loss_fn(output, data['y']) \n        loss_value.backward()\n        optimizer.step()\n        # сохранение истории\n        train_loss += loss_value.item()\n        if output.dim() > 1:\n            pred.append(output.argmax(1))\n        else:\n            pred.append(output >= 0.5)\n        gt.append(data['y'])\n    scheduler.step()\n\n    end_time = time.time()\n    epoch_time = end_time - start_time\n\n    num_batches = dataset['train']['y'].shape[0] // batch_size + 1\n    pred = torch.cat(pred)\n    gt = torch.cat(gt)\n    train_accuracy = (pred == gt).float().mean().item()\n\n    return train_loss / num_batches, train_accuracy, epoch_time # с нормировкой\n    \ndef validate(model, device, dataset, loss_fn, part='val'):\n    model.eval()\n    model.to(device)\n    val_loss = 0.0\n\n    pred = []\n    gt = [] # настоящие таргеты\n\n    dataset_name = dataset['info']['id'].split('--')[0]\n    task_type = dataset['info']['task_type']\n    batch_size = BATCH_SIZES[dataset_name]\n\n    with torch.no_grad():\n        start_time = time.time()\n        for data in delu.iter_batches(dataset[part], shuffle=False, batch_size=batch_size):\n            for key, tensor in data.items():\n                data[key] = tensor.to(device)\n            output = apply_model(data, model)\n            if task_type == 'multiclass':\n                data['y'] = data['y'].long()\n            val_loss += loss_fn(output, data['y']).item()\n            if output.dim() > 1:\n                pred.append(output.argmax(1))\n            else:\n                pred.append(output >= 0.5)\n            gt.append(data['y'])\n        end_time = time.time()\n        val_time = end_time - start_time\n        \n\n    num_batches = dataset[part]['y'].shape[0] // batch_size + 1\n    pred = torch.cat(pred)\n    gt = torch.cat(gt)\n    val_accuracy = (pred == gt).float().mean().item()\n\n    return val_loss / num_batches, val_accuracy, val_time # с нормировкой","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:09:46.782790Z","iopub.execute_input":"2025-04-14T11:09:46.783100Z","iopub.status.idle":"2025-04-14T11:09:46.795892Z","shell.execute_reply.started":"2025-04-14T11:09:46.783078Z","shell.execute_reply":"2025-04-14T11:09:46.795246Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Возможно ``model.to`` лучше вызывать 1 раз.","metadata":{}},{"cell_type":"code","source":"def train(\n    epochs, model, model_emb_name,\n    device, dataset, loss_fn,\n    optimizer,\n    optimizer_name=None,\n):\n    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n    model.to(device)\n    dataset_name = dataset['info']['id'].split('--')[0]\n    task_type = dataset['info']['task_type']\n    batch_size = BATCH_SIZES[dataset_name]\n\n    train_times = []\n    val_times = []\n    for epoch in tqdm(range(epochs), desc = f'{model_emb_name}_{optimizer_name} on {dataset_name}'):\n        train_loss, train_acc, train_time = train_epoch(model, device, dataset, loss_fn, optimizer, scheduler)\n        val_loss, val_acc, val_time = validate(model, device, dataset, loss_fn)\n\n        wandb.log({\n            'epoch' : epoch,\n            'train_loss' : train_loss,\n            'train_acc' : train_acc,\n            'val_loss' : val_loss,\n            'val_acc' : val_acc,\n            'lr' : scheduler.get_last_lr()[0]\n        })\n        train_times.append(train_time)\n        val_times.append(val_time)\n\n    # размерность входа backbone\n    in_features = dataset['train']['X_num'].shape[1]  # Количество числовых признаков\n    if 'X_cat' in dataset['train']:\n        in_features += dataset['train']['X_cat'].shape[1]  # Добавляем категориальные признаки\n\n    \n    wandb.log({\n        'train_epoch_time' : sum(train_times) / epochs,\n        'val_epoch_time' : sum(val_times) / epochs,\n        'num_params' : utils.count_parameters(model),\n        'in_features' : in_features,\n        'out_features' : (1 if task_type != 'multiclass' else dataset['info']['n_classes'])\n        # ширины и так будут залоггированы\n    })","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:09:48.558173Z","iopub.execute_input":"2025-04-14T11:09:48.558519Z","iopub.status.idle":"2025-04-14T11:09:48.565457Z","shell.execute_reply.started":"2025-04-14T11:09:48.558497Z","shell.execute_reply":"2025-04-14T11:09:48.564647Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Подготовку модели нужно переписать на языке ``wandb.config``. Все параметры, кроме ``hidden_layers`` вместо ``n_layers``, остаются такими же.","metadata":{}},{"cell_type":"code","source":"def model_init_preparation(config, dataset, model_name, emb_name):\n    dataset_info = dataset['info']\n    num_cont_cols = dataset['train']['X_num'].shape[1]\n    num_cat_cols = 0\n    if dataset_info['n_cat_features'] > 0:\n        num_cat_cols = dataset['train']['X_cat'].shape[1]\n    num_classes = 1\n    if dataset_info['task_type'] == 'multiclass':\n        num_classes = dataset_info['n_classes']\n        \n\n    # создание модели\n    layer_widths = list(range(config['hidden_layers'] + 2))\n    \n    if emb_name != 'none':\n        layer_widths[0] = num_cont_cols * config['d_embedding'] + num_cat_cols\n    else:\n        layer_widths[0] = num_cont_cols + num_cat_cols\n    layer_widths[1:-1] = [config['layer_width'] for i in range(config['hidden_layers'])] #скрытые слои\n    layer_widths[-1] = num_classes\n            \n    if model_name == 'kan':\n        backbone = KAN(layer_widths, grid_size=config['grid_size'], batch_norm=True)\n    elif model_name == 'mlp':\n        dropout = (config['dropout'] if config['use_dropout'] else 0)\n        backbone = MLP(layer_widths, dropout)\n    \n    # создание эмбеддингов\n    if emb_name == 'piecewiselinearq' or emb_name == 'PLE-Q':\n        bins = rtdl_num_embeddings.compute_bins(dataset['train']['X_num'], n_bins=config['d_embedding'])\n    elif emb_name == 'piecewiselineart' or emb_name == 'PLE-T': # это мы  больше не используем\n        tree_kwargs = {'min_samples_leaf': 64, 'min_impurity_decrease': 1e-4} #возможно стоит тюнить\n        bins = rtdl_num_embeddings.compute_bins(X=dataset['train']['X_num'], y=dataset['train']['y'], n_bins=config['d_embedding'], regression=True, tree_kwargs=tree_kwargs)\n    else:\n        bins = None\n            \n    task_type = dataset_info['task_type']\n    loss_fn = None\n    \n    if task_type == 'binclass':\n        loss_fn = F.binary_cross_entropy_with_logits\n    elif task_type == 'multiclass':\n        loss_fn = F.cross_entropy\n    else:\n        loss_fn =  F.mse_loss\n        \n    return layer_widths, backbone, bins, loss_fn\n    \n","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:09:50.044711Z","iopub.execute_input":"2025-04-14T11:09:50.045014Z","iopub.status.idle":"2025-04-14T11:09:50.052970Z","shell.execute_reply.started":"2025-04-14T11:09:50.044992Z","shell.execute_reply":"2025-04-14T11:09:50.052284Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Функцию, запускающую модель, дополним созданием определеннного ``optimizer``. Это тоже переписали на язык ``wandb.config``.","metadata":{}},{"cell_type":"code","source":"def get_optimizer(optim_name, model_params, config):\n    optim_class = OPTIMIZERS[optim_name]\n    optim_kwargs = {'lr' : config['lr']}\n    if optim_name != 'muon': # это на будущее\n        optim_kwargs['weight_decay'] = config['weight_decay']\n    return optim_class(model_params, **optim_kwargs)","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:09:51.431362Z","iopub.execute_input":"2025-04-14T11:09:51.431932Z","iopub.status.idle":"2025-04-14T11:09:51.435757Z","shell.execute_reply.started":"2025-04-14T11:09:51.431907Z","shell.execute_reply":"2025-04-14T11:09:51.435034Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"\n\nФункция для тюнинга!","metadata":{}},{"cell_type":"code","source":"def wandb_tuning(project_name, dataset_name, \n                 model_name, emb_name, optim_name, \n                 dataset, num_epochs=10, num_trials=30):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    dataset_info = dataset['info']\n    num_cont_cols = dataset['train']['X_num'].shape[1]\n    sweep_name = f'tuning {model_name}_{emb_name}_{optim_name} on {dataset_name}'\n    sweep_config = get_sweep_config(model_name, emb_name, dataset_info['task_type'], \n                                    f'tuning {model_name}_{emb_name}_{optim_name} on {dataset_name}')\n    \n    # просто оборачиваем нашу train\n    def sweep_wrapper():\n        with wandb.init(\n            project=f'{project_name}',\n            group=f'dataset_{dataset_name}',\n            tags=[f'model_{model_name}', f'emb_{emb_name}', f'optim_{optim_name}', f'dataset_{dataset_name}', 'tuning'],\n            config=sweep_config\n        ) as run:\n            config = wandb.config\n            _, backbone, bins, loss_fn = model_init_preparation(\n                config=config,\n                dataset=dataset,\n                model_name=model_name,\n                emb_name=emb_name\n            )\n            model = ModelWithEmbedding(\n                n_cont_features=num_cont_cols,\n                d_embedding=config.get('d_embedding', None),\n                emb_name=emb_name,\n                backbone_model=backbone,\n                bins=bins, \n                sigma=config.get('sigma', None)\n            )\n            train(\n                epochs=num_epochs,\n                model=model,\n                model_emb_name=f'{model_name}_{emb_name}_{optim_name}',\n                device=device,\n                dataset=dataset,\n                loss_fn=loss_fn,\n                optimizer=get_optimizer(optim_name, model.parameters(), config),\n                optimizer_name = optim_name\n            )\n    sweep_config = get_sweep_config(model_name, emb_name, task_type=dataset_info['task_type'])\n    sweep_id = wandb.sweep(sweep=sweep_config,\n                           project=f'{project_name}',\n                           entity='georgy-bulgakov') \n    wandb.agent(sweep_id, sweep_wrapper, count=num_trials)\n    return sweep_id","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:09:52.046164Z","iopub.execute_input":"2025-04-14T11:09:52.046418Z","iopub.status.idle":"2025-04-14T11:09:52.053519Z","shell.execute_reply.started":"2025-04-14T11:09:52.046399Z","shell.execute_reply":"2025-04-14T11:09:52.052845Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Функция для тестирования на разных сидах.","metadata":{}},{"cell_type":"code","source":"def test_best_model(best_params, project_name, dataset_name, model_name, emb_name, optim_name, dataset, num_epochs=10):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    num_cont_cols = dataset['train']['X_num'].shape[1]\n    d_embedding = best_params.get('d_embedding', None)\n    sigma = best_params.get('sigma', None)\n\n    # подготовка логирования\n    test_accuracies = []\n    test_losses = []\n    test_times = []\n    train_times = []\n\n    testing_config = get_test_config(model_name, emb_name, dataset['info']['task_type'], \n                                    f'testing {model_name}_{emb_name}_{optim_name} on {dataset_name}')\n    # обертка тестирования\n    def test_wrapper():\n        with wandb.init(\n            project=f'{project_name}',\n            group=f'dataset_{dataset_name}',\n            tags=[f'model_{model_name}', f'emb_{emb_name}', f'optim_{optim_name}', f'dataset_{dataset_name}', 'testing'],\n            config=testing_config\n        ) as run:\n            config = wandb.config\n            # seed + подготовка модели\n            utils.seed_everything(config['seed'])\n            layers, backbone, bins, loss_fn = model_init_preparation(\n                config=best_params,\n                dataset=dataset,\n                model_name=model_name,\n                emb_name=emb_name\n            )\n            model = ModelWithEmbedding(\n                n_cont_features=num_cont_cols,\n                d_embedding=d_embedding,\n                emb_name=emb_name,\n                backbone_model=backbone,\n                bins=bins, \n                sigma=sigma\n            )\n\n            start_time = time.time()\n            train(\n                epochs=num_epochs,\n                model=model,\n                model_emb_name=f'{model_name}_{emb_name}_{optim_name}',\n                device=device,\n                dataset=dataset,\n                loss_fn=loss_fn,\n                optimizer=get_optimizer(optim_name, model.parameters(), best_params)\n            )\n            end_time = time.time()\n            test_loss, test_acc, test_time = validate(model, device, dataset, loss_fn, 'test')\n            # Логируем результаты для каждого сида\n            run.log({\n                'test_loss': test_loss,\n                'test_acc': test_acc,\n                'full_train_time': end_time - start_time,\n                'test_time': test_time,\n                'seed': config['seed']\n            })\n\n            test_accuracies.append(test_acc)\n            test_losses.append(test_loss)\n            test_times.append(test_time)\n            train_times.append(end_time - start_time)\n\n    test_id = wandb.sweep(sweep=testing_config,\n                           project=f'{project_name}',\n                           entity='georgy-bulgakov') \n    wandb.agent(test_id, test_wrapper)\n\n    # Создаем финальный run для агрегированных результатов\n    with wandb.init(\n        project=f\"{project_name}\",\n        group=f'dataset_{dataset_name}',\n        name=\"aggregated_results\",\n        tags=[f'model_{model_name}', f'emb_{emb_name}', f'optim_{optim_name}', f'dataset_{dataset_name}', 'testing'],\n        config=best_params\n    ) as run:\n        stats = {\n            'model' : f'{model_name}_{emb_name}_{optim_name}',\n            'mean_test_acc': np.mean(test_accuracies),\n            'std_test_acc': np.std(test_accuracies),\n            'mean_test_loss': np.mean(test_losses),\n            'std_test_loss': np.std(test_losses),\n            'mean_test_time': np.mean(test_times),\n            'mean_train_time': np.mean(train_times),\n            'all_test_accs': test_accuracies,\n            'all_test_losses': test_losses,\n            'all_test_times': test_times,\n            'all_train_times': train_times\n        }\n        \n        run.log(stats)\n        stats['name'] = f'{model_name}_{emb_name}_{optim_name}'\n        \n    keys = ['model', 'mean_test_acc', 'std_test_acc', 'mean_test_loss', 'std_test_loss', 'mean_test_time', 'mean_train_time']\n    return {key : stats[key] for key in keys} # чисто технически для удобства","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:13:29.096902Z","iopub.execute_input":"2025-04-14T11:13:29.097378Z","iopub.status.idle":"2025-04-14T11:13:29.110731Z","shell.execute_reply.started":"2025-04-14T11:13:29.097353Z","shell.execute_reply":"2025-04-14T11:13:29.109920Z"},"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>in_features</td><td>▁</td></tr><tr><td>lr</td><td>█▅▂▁</td></tr><tr><td>num_params</td><td>▁</td></tr><tr><td>out_features</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁</td></tr><tr><td>train_epoch_time</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄▂▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁</td></tr><tr><td>val_epoch_time</td><td>▁</td></tr><tr><td>val_loss</td><td>█▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>in_features</td><td>13</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>num_params</td><td>2156</td></tr><tr><td>out_features</td><td>1</td></tr><tr><td>train_acc</td><td>0.79625</td></tr><tr><td>train_epoch_time</td><td>0.5413</td></tr><tr><td>train_loss</td><td>0.66448</td></tr><tr><td>val_acc</td><td>0.79625</td></tr><tr><td>val_epoch_time</td><td>0.04435</td></tr><tr><td>val_loss</td><td>0.67806</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">eager-sweep-3</strong> at: <a href='https://wandb.ai/georgy-bulgakov/TAB-KAN%20study/runs/lvfx6tm6' target=\"_blank\">https://wandb.ai/georgy-bulgakov/TAB-KAN%20study/runs/lvfx6tm6</a><br> View project at: <a href='https://wandb.ai/georgy-bulgakov/TAB-KAN%20study' target=\"_blank\">https://wandb.ai/georgy-bulgakov/TAB-KAN%20study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250414_111318-lvfx6tm6/logs</code>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from IPython.display import clear_output\ndef run_single_model(project_name, dataset_name, model_name, emb_name, optim_name, dataset, num_epochs):\n    sweep_id = wandb_tuning(project_name, dataset_name, model_name, emb_name, optim_name, dataset, num_epochs)\n    clear_output(wait=True)\n    # вспоминаем лучшие параметры\n    api = wandb.Api()\n    sweep = api.sweep(f'georgy-bulgakov/{project_name}/{sweep_id}')\n    runs = sweep.runs\n    best_run = None\n    if dataset['info']['task_type'] == 'regression':\n        best_run = min(runs, key=lambda run : run.summary.get('val_loss'))\n    else:\n        best_run = max(runs, key=lambda run : run.summary.get('val_acc'))\n    best_params = best_run.config\n    \n    stats = test_best_model(best_params, project_name, dataset_name, model_name, emb_name, optim_name, dataset, num_epochs)\n    return stats, best_params","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:13:32.057959Z","iopub.execute_input":"2025-04-14T11:13:32.058235Z","iopub.status.idle":"2025-04-14T11:13:32.062828Z","shell.execute_reply.started":"2025-04-14T11:13:32.058217Z","shell.execute_reply":"2025-04-14T11:13:32.061987Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import pandas as pd\nimport pickle\ndef run_single_dataset(project_name, dataset_name, optim_names, emb_names, model_names, num_epochs):\n    # dataset_type = dataset_info['type']\n    dataset = utils.load_dataset(dataset_name)\n    results = []\n    pkl_logs = {}\n\n    for model_name in model_names: # можно оставить только kan, тогда model_names = ['kan']\n        for optim_name in optim_names:\n            for emb_name in emb_names:\n                stats, best_params = run_single_model(project_name, dataset_name, model_name, emb_name, optim_name, dataset, num_epochs)\n                results.append(stats)\n                clear_output(wait=True)\n                pkl_logs[f'{model_name}_{emb_name}_{optim_name}'] = (stats | best_params)\n\n    \n    with wandb.init(\n        project=project_name,\n        group=f'dataset_{dataset_name}',\n        name='models_comparison'\n    ) as run:\n        run.log({\n            'Final Table' : wandb.Table(dataframe=pd.DataFrame(results))\n        })\n\n    with open(f'{dataset_name}.pkl', 'wb') as f:\n        pickle.dump(pkl_logs, f)\n    return results\n\n","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:13:33.361421Z","iopub.execute_input":"2025-04-14T11:13:33.361706Z","iopub.status.idle":"2025-04-14T11:13:33.367282Z","shell.execute_reply.started":"2025-04-14T11:13:33.361685Z","shell.execute_reply":"2025-04-14T11:13:33.366439Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"optim_names = ['adamw']\nmodel_names = ['kan']\nemb_names = ['none', 'periodic']\nproject_name = 'TAB-KAN study'\n\nfor dataset_name in ['churn', 'eye']:\n    run_single_dataset(project_name, dataset_name, optim_names, emb_names, model_names, 10)","metadata":{"execution":{"iopub.status.busy":"2025-04-14T11:13:34.031968Z","iopub.execute_input":"2025-04-14T11:13:34.032248Z","iopub.status.idle":"2025-04-14T11:33:47.526981Z","shell.execute_reply.started":"2025-04-14T11:13:34.032226Z","shell.execute_reply":"2025-04-14T11:33:47.526434Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'TAB-KAN study' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250414_113336-wxubdpmh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/georgy-bulgakov/TAB-KAN%20study/runs/wxubdpmh' target=\"_blank\">models_comparison</a></strong> to <a href='https://wandb.ai/georgy-bulgakov/TAB-KAN%20study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/georgy-bulgakov/TAB-KAN%20study/sweeps/ucy0isxc' target=\"_blank\">https://wandb.ai/georgy-bulgakov/TAB-KAN%20study/sweeps/ucy0isxc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/georgy-bulgakov/TAB-KAN%20study' target=\"_blank\">https://wandb.ai/georgy-bulgakov/TAB-KAN%20study</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/georgy-bulgakov/TAB-KAN%20study/sweeps/ucy0isxc' target=\"_blank\">https://wandb.ai/georgy-bulgakov/TAB-KAN%20study/sweeps/ucy0isxc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/georgy-bulgakov/TAB-KAN%20study/runs/wxubdpmh' target=\"_blank\">https://wandb.ai/georgy-bulgakov/TAB-KAN%20study/runs/wxubdpmh</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">models_comparison</strong> at: <a href='https://wandb.ai/georgy-bulgakov/TAB-KAN%20study/runs/wxubdpmh' target=\"_blank\">https://wandb.ai/georgy-bulgakov/TAB-KAN%20study/runs/wxubdpmh</a><br> View project at: <a href='https://wandb.ai/georgy-bulgakov/TAB-KAN%20study' target=\"_blank\">https://wandb.ai/georgy-bulgakov/TAB-KAN%20study</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250414_113336-wxubdpmh/logs</code>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
