{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install delu\n",
    "!pip install ucimlrepo\n",
    "!pip install gdown\n",
    "!pip install rtdl_num_embeddings\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from typing import Literal, Optional\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import rtdl_num_embeddings\n",
    "import delu\n",
    "from IPython.display import FileLink\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# для тестов\n",
    "# sys.path.append('/kaggle/working/benchmarking-KAN/data')\n",
    "# sys.path.append('/kaggle/working/benchmarking-KAN/kan')\n",
    "# sys.path.append('/kaggle/working/benchmarking-KAN/utils')\n",
    "from efficient_kan import KAN\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я вынес в файл ``utils`` функции ``count_parameters``,``load_dataset``, ``seed_everything``,  ``write_results``. \n",
    "Модели не меняем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, layers, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        total_layers = []\n",
    "        for n_in, n_out in zip(layers[:-2], layers[1:-1]):\n",
    "            total_layers.append(nn.Linear(n_in, n_out))\n",
    "            total_layers.append(nn.SiLU(inplace=False))\n",
    "            total_layers.append(nn.Dropout(dropout, inplace=False))\n",
    "        total_layers.append(nn.Linear(layers[-2], layers[-1])) # выходной слой\n",
    "\n",
    "        self.classifier = nn.Sequential(*total_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['adult', 'california', 'churn', 'covtype', 'eye', 'fb-comments',\n",
    "           'gesture', 'higgs-small', 'house', 'microsoft', 'otto', 'santander']\n",
    "\n",
    "\n",
    "BATCH_SIZES = {'gesture' : 128, 'churn' : 128, 'california' : 256, 'house' : 256, 'adult' : 256, 'otto' : 512, \n",
    "               'higgs-small' : 512, 'fb-comments' : 512, 'santander' : 1024, 'covtype' : 1024, 'microsoft' : 1024, 'eye': 128}\n",
    "\n",
    "REGRESSION = ['house', 'fb-comments', 'microsoft', 'california']\n",
    "MULTICLASS = ['covtype', 'eye', 'gesture', 'otto']\n",
    "BINCLASS = ['adult', 'churn', 'higgs-small', 'santander']\n",
    "\n",
    "OPTIMIZERS = {'AdamW' : torch.optim.AdamW,\n",
    "              'MARS' : MARS,\n",
    "              'AdEMAMix' : AdEMAMix,\n",
    "               'Muon' : Muon}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cont_features,\n",
    "        d_embedding,\n",
    "        emb_name,\n",
    "        backbone_model,\n",
    "        bins, sigma=None # словарь всех необязательных параметров, например sigma, bins\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_embedding = d_embedding\n",
    "        self.emb_name = emb_name\n",
    "        \n",
    "        if emb_name == 'periodic':\n",
    "            self.cont_embeddings = rtdl_num_embeddings.PeriodicEmbeddings(\n",
    "                n_cont_features, d_embedding, frequency_init_scale=sigma, lite=True\n",
    "            )\n",
    "            \n",
    "        if emb_name == 'piecewiselinearq' or emb_name == 'piecewiselineart':\n",
    "            self.cont_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "                d_embedding=d_embedding, activation=False, version='B', bins=bins\n",
    "            )\n",
    "\n",
    "        self.backbone = backbone_model\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x_num : Tensor,\n",
    "        x_cat : Optional[Tensor] = None\n",
    "    ) -> Tensor:\n",
    "        x = []\n",
    "        # Step 1. Embed the continuous features.\n",
    "        # Flattening is needed for MLP-like models.\n",
    "        if self.emb_name != 'none':\n",
    "              x_num = self.cont_embeddings(x_num)\n",
    "        x.append(x_num.flatten(1))\n",
    "        \n",
    "        #categorical features do not need embeddings\n",
    "        if x_cat is not None:\n",
    "            x.append(x_cat.flatten(1))\n",
    "        \n",
    "        x = torch.column_stack(x)\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ``train`` теперь передаем целиком ``optimizer``.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch.optim import AdamW, Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR, CosineAnnealingLR\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "\n",
    "def apply_model(batch: dict[str, Tensor], model) -> Tensor:\n",
    "    return model(batch['X_num'], batch.get('X_cat')).squeeze(-1)\n",
    "\n",
    "\n",
    "def train(\n",
    "    epochs, model, model_emb_name,\n",
    "    device, dataset, loss_fn,\n",
    "    optimizer,\n",
    "    optimizer_name=None\n",
    "):\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    model.to(device)\n",
    "    dataset_name = dataset['info']['id'].split('--')[0]\n",
    "    task_type = dataset['info']['task_type']\n",
    "    batch_size = BATCH_SIZES[dataset_name]\n",
    "\n",
    "    times = []\n",
    "    for epoch in tqdm(range(epochs), desc = f'{model_emb_name}_{optimizer_name} on {dataset_name}'):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "\n",
    "        for data in delu.iter_batches(dataset['train'], shuffle=True, batch_size=batch_size):\n",
    "            for key, tensor in data.items():\n",
    "                data[key] = tensor.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = apply_model(data, model)\n",
    "            if task_type == 'multiclass':\n",
    "                data['y'] = data['y'].long()\n",
    "            loss_value = loss_fn(output, data['y']) # здесь был каст к типу long (добавил обратно, без него не работает)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "\n",
    "    # Return the average times of training epochs\n",
    "    t = sum(times)/len(times)\n",
    "    return t\n",
    "\n",
    "def validate(model, device, dataset, loss_fn, part='val'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    val_loss = 0.0\n",
    "\n",
    "    pred = []\n",
    "    gt = [] # настоящие таргеты\n",
    "\n",
    "    dataset_name = dataset['info']['id'].split('--')[0]\n",
    "    task_type = dataset['info']['task_type']\n",
    "    batch_size = BATCH_SIZES[dataset_name]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in delu.iter_batches(dataset[part], shuffle=False, batch_size=batch_size):\n",
    "            for key, tensor in data.items():\n",
    "                data[key] = tensor.to(device)\n",
    "            output = apply_model(data, model)\n",
    "            if task_type == 'multiclass':\n",
    "                data['y'] = data['y'].long()\n",
    "            val_loss += loss_fn(output, data['y']).item()\n",
    "            if output.dim() > 1:\n",
    "                pred.append(output.argmax(1))\n",
    "            else:\n",
    "                pred.append(output >= 0.5)\n",
    "            gt.append(data['y'])\n",
    "\n",
    "    num_batches = dataset[part]['y'].shape[0] // batch_size + 1\n",
    "    pred = torch.cat(pred)\n",
    "    gt = torch.cat(gt)\n",
    "    val_accuracy = (pred == gt).float().mean().item()\n",
    "\n",
    "    return val_loss / num_batches, val_accuracy # с нормировкой\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбор параметров не меняем, тюним архитектуру сеток, ``lr``, ``weight_decay``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_params(trial, opt_params, model_name, emb_name):\n",
    "    #можно добавить что-то/убрать\n",
    "    params = {'n_layers': trial.suggest_int('n_layers', 1, opt_params['max_n_layer']),\n",
    "            'layer_width': trial.suggest_int('layer_width', opt_params['min_layer_width'], opt_params['max_layer_width'], step=opt_params['layer_width_step']),\n",
    "            'lr' : trial.suggest_float('lr', opt_params['min_lr'], opt_params['max_lr'], log=True),\n",
    "            'weight_decay' : trial.suggest_float('weight_decay', opt_params['min_weight_decay'], opt_params['max_weight_decay'], log=True)}\n",
    "    \n",
    "    params['d_embedding'] = (trial.suggest_int('d_embedding', opt_params['min_d_embedding'], opt_params['max_d_embedding']) \n",
    "                            if emb_name != 'none'\n",
    "                            else 0)\n",
    "    \n",
    "    if model_name == 'mlp':\n",
    "        use_dropout = trial.suggest_categorical('use_dropout', [True, False])\n",
    "        params['use_dropout'] = use_dropout\n",
    "        params['dropout'] = (trial.suggest_float('dropout', 0, 0.5) if use_dropout else 0)\n",
    "    params['sigma'] = (trial.suggest_float('sigma', opt_params['min_sigma'], opt_params['max_sigma'], log=True) if emb_name == 'periodic' else None) #дисперсия инициализации весов plr\n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменял размеры ``KAN`` и огрубил шаг в ширине до 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_optuna_params(dataset_name, model_name, emb_name):\n",
    "    #здесь можно добавить различное пространство гиперпараметров для разных датасетов (пока возвращает все то же самое)\n",
    "    params = {'max_n_layer' : 4,\n",
    "              'min_layer_width' : (1 if model_name == 'mlp' else 1),\n",
    "              'max_layer_width' : (1024 if model_name == 'mlp' else 64),\n",
    "              'layer_width_step' : (16 if model_name == 'mlp' else 4),\n",
    "              'min_lr' : 1e-4,\n",
    "              'max_lr': 1e-2,\n",
    "              'min_weight_decay' : 5e-4,\n",
    "              'max_weight_decay' : 5e-2}\n",
    "\n",
    "    if emb_name != 'none':\n",
    "        params['max_d_embedding'] = 128\n",
    "        params['min_d_embedding'] = 2\n",
    "    \n",
    "    if emb_name == 'periodic':\n",
    "        params['min_sigma'] = 0.01\n",
    "        params['max_sigma'] = 100\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init_preparation(params, dataset, num_classes, model_name, emb_name):\n",
    "    dataset_info = dataset['info']\n",
    "    num_cont_cols = dataset['train']['X_num'].shape[1]\n",
    "    num_cat_cols = 0\n",
    "    if dataset_info['n_cat_features'] > 0:\n",
    "        num_cat_cols = dataset['train']['X_cat'].shape[1]\n",
    "\n",
    "    # создание модели\n",
    "    layer_widths = list(range(params['n_layers'] + 2))\n",
    "    \n",
    "    if emb_name != 'none':\n",
    "        layer_widths[0] = num_cont_cols * params['d_embedding'] + num_cat_cols\n",
    "    else:\n",
    "        layer_widths[0] = num_cont_cols + num_cat_cols\n",
    "    layer_widths[1:-1] = [params['layer_width'] for i in range(params['n_layers'])] #скрытые слои\n",
    "    layer_widths[-1] = num_classes\n",
    "            \n",
    "    if model_name == 'kan':\n",
    "        backbone = KAN(layer_widths, grid_size=15, batch_norm=True)\n",
    "    elif model_name == 'mlp':\n",
    "        dropout = (params['dropout'] if params['use_dropout'] else 0)\n",
    "        backbone = MLP(layer_widths, dropout)\n",
    "    \n",
    "    # создание эмбеддингов\n",
    "    if emb_name == 'piecewiselinearq':\n",
    "        bins = rtdl_num_embeddings.compute_bins(dataset['train']['X_num'], n_bins=params['d_embedding'])\n",
    "    elif emb_name == 'piecewiselineart': # это мы  больше не используем\n",
    "        tree_kwargs = {'min_samples_leaf': 64, 'min_impurity_decrease': 1e-4} #возможно стоит тюнить\n",
    "        bins = rtdl_num_embeddings.compute_bins(X=dataset['train']['X_num'], y=dataset['train']['y'], n_bins=params['d_embedding'], regression=True, tree_kwargs=tree_kwargs)\n",
    "    else:\n",
    "        bins = None\n",
    "            \n",
    "    task_type = dataset_info['task_type']\n",
    "    loss_fn = None\n",
    "    \n",
    "    if task_type == 'binclass':\n",
    "        loss_fn = F.binary_cross_entropy_with_logits\n",
    "    elif task_type == 'multiclass':\n",
    "        loss_fn = F.cross_entropy\n",
    "    else:\n",
    "        loss_fn =  F.mse_loss\n",
    "        \n",
    "    return layer_widths, backbone, bins, loss_fn\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функцию, запускающую модель, дополним созданием определеннного ``optimizer``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_model(pkl_path, model_name, emb_name, optim_name, dataset, num_epochs):\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dataset_info = dataset['info']\n",
    "    \n",
    "    opt_params = read_optuna_params(dataset_info['name'], model_name, emb_name)\n",
    "\n",
    "    num_classes = 1\n",
    "    if dataset_info['task_type'] == 'multiclass':\n",
    "        num_classes = dataset_info['n_classes']\n",
    "    num_cont_cols = dataset['train']['X_num'].shape[1]\n",
    "    \n",
    "    num_params = []\n",
    "    training_time_per_epoch = []\n",
    "\n",
    "    # сохранили КЛАСС этого оптимайзера\n",
    "    optim_class = OPTIMIZERS[optim_name]\n",
    "    def objective(trial):\n",
    "        #возьмем гипперпараметры из оптуны\n",
    "        params = suggest_params(trial, opt_params, model_name=model_name, emb_name=emb_name)\n",
    "        \n",
    "        # создаем модель и оптимайзер\n",
    "        _, backbone, bins, loss_fn = model_init_preparation(\n",
    "            params=best_params,\n",
    "            dataset=dataset,\n",
    "            num_classes=num_classes,\n",
    "            model_name=model_name,\n",
    "            emb_name=emb_name\n",
    "        )\n",
    "        model = ModelWithEmbedding(\n",
    "            num_cont_features=num_cont_cols,  # Количество числовых признаков\n",
    "            d_embedding=params['d_embedding'],    # Размерность эмбеддингов\n",
    "            emb_name=emb_name,                # Тип используемого эмбеддинга\n",
    "            backbone_model=backbone,                # Базовая архитектура модели\n",
    "            bins=bins,                    # Параметры бининга для числовых признаков\n",
    "            sigma=params['sigma']          # Параметр sigma для Gaussian слоев\n",
    "        )\n",
    "        model.to(device) \n",
    "        optimizer = optim_class(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "        \n",
    "        # обучаем модель при данных параметрах\n",
    "        epoch_training_time = train(\n",
    "            epochs=num_epochs,\n",
    "            model=model,\n",
    "            model_emb_name=f'{model_name}_{emb_name}',\n",
    "            device=device,                               \n",
    "            dataset=dataset,                    \n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer, \n",
    "            optimizer_name=optim_name\n",
    "        )\n",
    "        training_time_per_epoch.append(epoch_training_time)\n",
    "        num_params.append(utils.count_parameters(model))\n",
    "\n",
    "        val_loss, val_accuracy = validate(model, device, dataset, loss_fn)\n",
    "        \n",
    "        return (val_loss if dataset_info['task_type'] == 'regression' else val_accuracy)\n",
    "\n",
    "    direction = ('minimize' if dataset_info['task_type'] == 'regression' else 'maximize')\n",
    "    \n",
    "    study = optuna.create_study(direction=direction)\n",
    "    study.optimize(objective, n_trials=70)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    layers, backbone, bins, loss_fn = model_init_preparation(\n",
    "        params=best_params,\n",
    "        dataset=dataset,\n",
    "        num_classes=num_classes,\n",
    "        model_name=model_name,\n",
    "        emb_name=emb_name\n",
    "    )\n",
    "    d_embedding = (best_params['d_embedding'] if emb_name != 'none' else 1)\n",
    "    sigma = (best_params['sigma'] if emb_name == 'periodic' else None)\n",
    "    lr = best_params['lr']\n",
    "    weight_decay = best_params['weight_decay']\n",
    "\n",
    "    \n",
    "    test_accuracies = []\n",
    "    test_losses = []\n",
    "    for s in range(10):\n",
    "        utils.seed_everything(s)\n",
    "        \n",
    "        model = ModelWithEmbedding(num_cont_cols, d_embedding, emb_name, backbone_model=backbone, bins=bins, sigma=sigma)\n",
    "        model.to(device)   \n",
    "        optimizer = optim_class(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        train(num_epochs, model, device, dataset, loss_fn, optimizer)\n",
    "        test_loss, test_accuracy = validate(model, device, dataset, loss_fn, part='test')\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "    utils.write_results(pkl_path, model_name, emb_name, optim_name, \n",
    "                        layers, num_epochs, num_params, best_params, \n",
    "                        test_accuracies, test_losses, training_time_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def run_single_dataset(dataset_name, optim_names, emb_names, model_names, num_epochs):\n",
    "    # dataset_type = dataset_info['type']\n",
    "    dataset = utils.load_dataset(dataset_name)\n",
    "    pkl_path = f'{dataset_name}.pkl'\n",
    "    for model_name in model_names: # можно оставить только kan, тогда model_names = ['kan']\n",
    "        for optim_name in optim_names:\n",
    "            for emb_name in emb_names:\n",
    "                run_single_model(pkl_path, model_name, emb_name, optim_name, dataset, num_epochs)\n",
    "                clear_output(wait=True)\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
