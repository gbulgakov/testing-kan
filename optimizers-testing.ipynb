{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-09T13:07:08.247824Z",
     "iopub.status.busy": "2025-04-09T13:07:08.247509Z",
     "iopub.status.idle": "2025-04-09T13:07:23.628172Z",
     "shell.execute_reply": "2025-04-09T13:07:23.627362Z",
     "shell.execute_reply.started": "2025-04-09T13:07:08.247797Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delu\n",
      "  Downloading delu-0.0.26-py3-none-any.whl.metadata (805 bytes)\n",
      "Requirement already satisfied: numpy<3,>=1.21 in /usr/local/lib/python3.10/dist-packages (from delu) (1.26.4)\n",
      "Requirement already satisfied: torch<3,>=1.9 in /usr/local/lib/python3.10/dist-packages (from delu) (2.5.1+cu121)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=1.9->delu) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.9->delu) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.21->delu) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.21->delu) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.21->delu) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.21->delu) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.21->delu) (2024.2.0)\n",
      "Downloading delu-0.0.26-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: delu\n",
      "Successfully installed delu-0.0.26\n",
      "Collecting ucimlrepo\n",
      "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2025.1.31)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2024.2.0)\n",
      "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
      "Installing collected packages: ucimlrepo\n",
      "Successfully installed ucimlrepo-0.0.7\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.17.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Collecting rtdl_num_embeddings\n",
      "  Downloading rtdl_num_embeddings-0.0.11-py3-none-any.whl.metadata (882 bytes)\n",
      "Requirement already satisfied: torch<3,>=1.12 in /usr/local/lib/python3.10/dist-packages (from rtdl_num_embeddings) (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=1.12->rtdl_num_embeddings) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.12->rtdl_num_embeddings) (3.0.2)\n",
      "Downloading rtdl_num_embeddings-0.0.11-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rtdl_num_embeddings\n",
      "Successfully installed rtdl_num_embeddings-0.0.11\n",
      "Cloning into 'testing-kan'...\n",
      "remote: Enumerating objects: 133, done.\u001b[K\n",
      "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
      "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
      "remote: Total 133 (delta 65), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (133/133), 65.33 KiB | 608.00 KiB/s, done.\n",
      "Resolving deltas: 100% (65/65), done.\n"
     ]
    }
   ],
   "source": [
    "!pip install delu\n",
    "!pip install ucimlrepo\n",
    "!pip install gdown\n",
    "!pip install rtdl_num_embeddings\n",
    "!git clone https://github.com/gbulgakov/testing-kan.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:10.239137Z",
     "iopub.status.busy": "2025-04-09T13:08:10.238837Z",
     "iopub.status.idle": "2025-04-09T13:08:10.244564Z",
     "shell.execute_reply": "2025-04-09T13:08:10.243626Z",
     "shell.execute_reply.started": "2025-04-09T13:08:10.239114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from typing import Literal, Optional\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import rtdl_num_embeddings\n",
    "import delu\n",
    "from IPython.display import FileLink\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# наши импорты\n",
    "sys.path.append('/kaggle/working/testing-kan/optimizers')\n",
    "sys.path.append('/kaggle/working/testing-kan')\n",
    "from ademamix import AdEMAMix\n",
    "from mars import MARS\n",
    "from muon import Muon\n",
    "from efficient_kan import KAN\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:07:54.726107Z",
     "iopub.status.busy": "2025-04-09T13:07:54.725615Z",
     "iopub.status.idle": "2025-04-09T13:07:59.798891Z",
     "shell.execute_reply": "2025-04-09T13:07:59.798048Z",
     "shell.execute_reply.started": "2025-04-09T13:07:54.726080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1s0w7gnhiwBCkF49Wdi_cUDpUtXlz2_6q\n",
      "To: /kaggle/working/eye.zip\n",
      "100%|█████████████████████████████████████████| 534k/534k [00:00<00:00, 101MB/s]\n"
     ]
    }
   ],
   "source": [
    "# !gdown 1xvRa_-OEeG6xNRYE5V5iAfTwyWM1NiLl # otto\n",
    "# !gdown 1tYyhbHdYs_8I9jvXznMoeUAfBzwitaax # house\n",
    "# !gdown 1hy1dOAL2SE-XZSuMcjLcVgml2CoYkF9q # higgs-small\n",
    "# !gdown 1hr076cK9QFxH6YZRg5V4av-H7IAve59r # gesture\n",
    "# !gdown 1ZNScy5fgqtgudT6MZ4EjLt1nwdqirtmX # fb-comments\n",
    "!gdown 1s0w7gnhiwBCkF49Wdi_cUDpUtXlz2_6q # eye\n",
    "# !gdown 1T04iP04UGVo95Om84ww1Ed8AFNziOaeY # covtype\n",
    "# !gdown 1GOkNlinj4zHVSNKbqjN1rR4cvsAf2IgR # churn\n",
    "# !gdown 11B-l4EasJkclK_Q-RBcxYfGJLSvz-v5c # california\n",
    "# !gdown 1p8uqDPMfRlFIc69m7iikS6wGkA6JGj1H # adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я вынес в файл ``utils`` функции ``count_parameters``,``load_dataset``, ``seed_everything``,  ``write_results``. \n",
    "Модели не меняем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:12.167186Z",
     "iopub.status.busy": "2025-04-09T13:08:12.166885Z",
     "iopub.status.idle": "2025-04-09T13:08:12.172232Z",
     "shell.execute_reply": "2025-04-09T13:08:12.171363Z",
     "shell.execute_reply.started": "2025-04-09T13:08:12.167163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASETS = ['adult', 'california', 'churn', 'covtype', 'eye', 'fb-comments',\n",
    "           'gesture', 'higgs-small', 'house', 'microsoft', 'otto', 'santander']\n",
    "\n",
    "\n",
    "BATCH_SIZES = {'gesture' : 128, 'churn' : 128, 'california' : 256, 'house' : 256, 'adult' : 256, 'otto' : 512, \n",
    "               'higgs-small' : 512, 'fb-comments' : 512, 'santander' : 1024, 'covtype' : 1024, 'microsoft' : 1024, 'eye': 128}\n",
    "\n",
    "REGRESSION = ['house', 'fb-comments', 'microsoft', 'california']\n",
    "MULTICLASS = ['covtype', 'eye', 'gesture', 'otto']\n",
    "BINCLASS = ['adult', 'churn', 'higgs-small', 'santander']\n",
    "\n",
    "OPTIMIZERS = {'adamw' : torch.optim.AdamW,\n",
    "              'mars' : MARS,\n",
    "              'ademamix' : AdEMAMix,\n",
    "              'muon' : Muon}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели не меняем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:13.604178Z",
     "iopub.status.busy": "2025-04-09T13:08:13.603863Z",
     "iopub.status.idle": "2025-04-09T13:08:13.609190Z",
     "shell.execute_reply": "2025-04-09T13:08:13.608217Z",
     "shell.execute_reply.started": "2025-04-09T13:08:13.604150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, layers, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        total_layers = []\n",
    "        for n_in, n_out in zip(layers[:-2], layers[1:-1]):\n",
    "            total_layers.append(nn.Linear(n_in, n_out))\n",
    "            total_layers.append(nn.SiLU(inplace=False))\n",
    "            total_layers.append(nn.Dropout(dropout, inplace=False))\n",
    "        total_layers.append(nn.Linear(layers[-2], layers[-1])) # выходной слой\n",
    "\n",
    "        self.classifier = nn.Sequential(*total_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:16.035575Z",
     "iopub.status.busy": "2025-04-09T13:08:16.035292Z",
     "iopub.status.idle": "2025-04-09T13:08:16.041786Z",
     "shell.execute_reply": "2025-04-09T13:08:16.040969Z",
     "shell.execute_reply.started": "2025-04-09T13:08:16.035555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelWithEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cont_features,\n",
    "        d_embedding,\n",
    "        emb_name,\n",
    "        backbone_model,\n",
    "        bins, sigma=None # словарь всех необязательных параметров, например sigma, bins\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_embedding = d_embedding\n",
    "        self.emb_name = emb_name\n",
    "        \n",
    "        if emb_name == 'periodic':\n",
    "            self.cont_embeddings = rtdl_num_embeddings.PeriodicEmbeddings(\n",
    "                n_cont_features, d_embedding, frequency_init_scale=sigma, lite=True\n",
    "            )\n",
    "            \n",
    "        if emb_name == 'piecewiselinearq' or emb_name == 'piecewiselineart':\n",
    "            self.cont_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n",
    "                d_embedding=d_embedding, activation=False, version='B', bins=bins\n",
    "            )\n",
    "\n",
    "        self.backbone = backbone_model\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x_num : Tensor,\n",
    "        x_cat : Optional[Tensor] = None\n",
    "    ) -> Tensor:\n",
    "        x = []\n",
    "        # Step 1. Embed the continuous features.\n",
    "        # Flattening is needed for MLP-like models.\n",
    "        if self.emb_name != 'none':\n",
    "              x_num = self.cont_embeddings(x_num)\n",
    "        x.append(x_num.flatten(1))\n",
    "        \n",
    "        #categorical features do not need embeddings\n",
    "        if x_cat is not None:\n",
    "            x.append(x_cat.flatten(1))\n",
    "        \n",
    "        x = torch.column_stack(x)\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ``train`` теперь передаем целиком ``optimizer``.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:18.971804Z",
     "iopub.status.busy": "2025-04-09T13:08:18.971441Z",
     "iopub.status.idle": "2025-04-09T13:08:18.983626Z",
     "shell.execute_reply": "2025-04-09T13:08:18.982627Z",
     "shell.execute_reply.started": "2025-04-09T13:08:18.971766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch.optim import AdamW, Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR, CosineAnnealingLR\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "\n",
    "def apply_model(batch: dict[str, Tensor], model) -> Tensor:\n",
    "    return model(batch['X_num'], batch.get('X_cat')).squeeze(-1)\n",
    "\n",
    "\n",
    "def train(\n",
    "    epochs, model, model_emb_name,\n",
    "    device, dataset, loss_fn,\n",
    "    optimizer,\n",
    "    optimizer_name=None\n",
    "):\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    model.to(device)\n",
    "    dataset_name = dataset['info']['id'].split('--')[0]\n",
    "    task_type = dataset['info']['task_type']\n",
    "    batch_size = BATCH_SIZES[dataset_name]\n",
    "\n",
    "    times = []\n",
    "    for epoch in tqdm(range(epochs), desc = f'{model_emb_name}_{optimizer_name} on {dataset_name}'):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "\n",
    "        for data in delu.iter_batches(dataset['train'], shuffle=True, batch_size=batch_size):\n",
    "            for key, tensor in data.items():\n",
    "                data[key] = tensor.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = apply_model(data, model)\n",
    "            if task_type == 'multiclass':\n",
    "                data['y'] = data['y'].long()\n",
    "            loss_value = loss_fn(output, data['y']) # здесь был каст к типу long (добавил обратно, без него не работает)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "\n",
    "    # Return the average times of training epochs\n",
    "    t = sum(times)/len(times)\n",
    "    return t\n",
    "\n",
    "def validate(model, device, dataset, loss_fn, part='val'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    val_loss = 0.0\n",
    "\n",
    "    pred = []\n",
    "    gt = [] # настоящие таргеты\n",
    "\n",
    "    dataset_name = dataset['info']['id'].split('--')[0]\n",
    "    task_type = dataset['info']['task_type']\n",
    "    batch_size = BATCH_SIZES[dataset_name]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for data in delu.iter_batches(dataset[part], shuffle=False, batch_size=batch_size):\n",
    "            for key, tensor in data.items():\n",
    "                data[key] = tensor.to(device)\n",
    "            output = apply_model(data, model)\n",
    "            if task_type == 'multiclass':\n",
    "                data['y'] = data['y'].long()\n",
    "            val_loss += loss_fn(output, data['y']).item()\n",
    "            if output.dim() > 1:\n",
    "                pred.append(output.argmax(1))\n",
    "            else:\n",
    "                pred.append(output >= 0.5)\n",
    "            gt.append(data['y'])\n",
    "        end_time = time.time()\n",
    "        val_time = start_time - end_time\n",
    "        \n",
    "\n",
    "    num_batches = dataset[part]['y'].shape[0] // batch_size + 1\n",
    "    pred = torch.cat(pred)\n",
    "    gt = torch.cat(gt)\n",
    "    val_accuracy = (pred == gt).float().mean().item()\n",
    "\n",
    "    return val_loss / num_batches, val_accuracy, val_time # с нормировкой\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбор параметров не меняем, тюним архитектуру сеток, ``lr``, ``weight_decay``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:22.215865Z",
     "iopub.status.busy": "2025-04-09T13:08:22.215522Z",
     "iopub.status.idle": "2025-04-09T13:08:22.222189Z",
     "shell.execute_reply": "2025-04-09T13:08:22.221144Z",
     "shell.execute_reply.started": "2025-04-09T13:08:22.215840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def suggest_params(trial, optuna_params, model_name, emb_name, optim_name):\n",
    "    #можно добавить что-то/убрать\n",
    "    params = {'n_layers': trial.suggest_int('n_layers', 1, optuna_params['max_n_layer']),\n",
    "            'layer_width': trial.suggest_int('layer_width', optuna_params['min_layer_width'], optuna_params['max_layer_width'], step=optuna_params['layer_width_step']),\n",
    "            'lr' : trial.suggest_float('lr', optuna_params['min_lr'], optuna_params['max_lr'], log=True)}\n",
    "    if optim_name != 'muon':\n",
    "        params['weight_decay'] = trial.suggest_float('weight_decay', optuna_params['min_weight_decay'], optuna_params['max_weight_decay'], log=True)\n",
    "    \n",
    "    params['d_embedding'] = (trial.suggest_int('d_embedding', optuna_params['min_d_embedding'], optuna_params['max_d_embedding']) \n",
    "                            if emb_name != 'none'\n",
    "                            else 0)\n",
    "    \n",
    "    if model_name == 'mlp':\n",
    "        use_dropout = trial.suggest_categorical('use_dropout', [True, False])\n",
    "        params['use_dropout'] = use_dropout\n",
    "        params['dropout'] = (trial.suggest_float('dropout', 0, 0.5) if use_dropout else 0)\n",
    "    params['sigma'] = (trial.suggest_float('sigma', optuna_params['min_sigma'], optuna_params['max_sigma'], log=True) if emb_name == 'periodic' else None) #дисперсия инициализации весов plr\n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменял размеры (ширину) ``KAN`` и огрубил шаг в ширине до 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:24.480437Z",
     "iopub.status.busy": "2025-04-09T13:08:24.480153Z",
     "iopub.status.idle": "2025-04-09T13:08:24.485451Z",
     "shell.execute_reply": "2025-04-09T13:08:24.484521Z",
     "shell.execute_reply.started": "2025-04-09T13:08:24.480415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_optuna_params(dataset_name, model_name, emb_name):\n",
    "    #здесь можно добавить различное пространство гиперпараметров для разных датасетов (пока возвращает все то же самое)\n",
    "    params = {'max_n_layer' : 4,\n",
    "              'min_layer_width' : (1 if model_name == 'mlp' else 1),\n",
    "              'max_layer_width' : (1024 if model_name == 'mlp' else 64),\n",
    "              'layer_width_step' : (16 if model_name == 'mlp' else 4),\n",
    "              'min_lr' : 1e-4,\n",
    "              'max_lr': 1e-2,\n",
    "              'min_weight_decay' : 5e-4, # для muon это не актуально, но оставим эти константы\n",
    "              'max_weight_decay' : 5e-2}\n",
    "\n",
    "    if emb_name != 'none':\n",
    "        params['max_d_embedding'] = 128\n",
    "        params['min_d_embedding'] = 2\n",
    "    \n",
    "    if emb_name == 'periodic':\n",
    "        params['min_sigma'] = 0.01\n",
    "        params['max_sigma'] = 100\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:28.614060Z",
     "iopub.status.busy": "2025-04-09T13:08:28.613742Z",
     "iopub.status.idle": "2025-04-09T13:08:28.621391Z",
     "shell.execute_reply": "2025-04-09T13:08:28.620418Z",
     "shell.execute_reply.started": "2025-04-09T13:08:28.614035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def model_init_preparation(params, dataset, num_classes, model_name, emb_name):\n",
    "    dataset_info = dataset['info']\n",
    "    num_cont_cols = dataset['train']['X_num'].shape[1]\n",
    "    num_cat_cols = 0\n",
    "    if dataset_info['n_cat_features'] > 0:\n",
    "        num_cat_cols = dataset['train']['X_cat'].shape[1]\n",
    "\n",
    "    # создание модели\n",
    "    layer_widths = list(range(params['n_layers'] + 2))\n",
    "    \n",
    "    if emb_name != 'none':\n",
    "        layer_widths[0] = num_cont_cols * params['d_embedding'] + num_cat_cols\n",
    "    else:\n",
    "        layer_widths[0] = num_cont_cols + num_cat_cols\n",
    "    layer_widths[1:-1] = [params['layer_width'] for i in range(params['n_layers'])] #скрытые слои\n",
    "    layer_widths[-1] = num_classes\n",
    "            \n",
    "    if model_name == 'kan':\n",
    "        backbone = KAN(layer_widths, grid_size=15, batch_norm=True)\n",
    "    elif model_name == 'mlp':\n",
    "        dropout = (params['dropout'] if params['use_dropout'] else 0)\n",
    "        backbone = MLP(layer_widths, dropout)\n",
    "    \n",
    "    # создание эмбеддингов\n",
    "    if emb_name == 'piecewiselinearq':\n",
    "        bins = rtdl_num_embeddings.compute_bins(dataset['train']['X_num'], n_bins=params['d_embedding'])\n",
    "    elif emb_name == 'piecewiselineart': # это мы  больше не используем\n",
    "        tree_kwargs = {'min_samples_leaf': 64, 'min_impurity_decrease': 1e-4} #возможно стоит тюнить\n",
    "        bins = rtdl_num_embeddings.compute_bins(X=dataset['train']['X_num'], y=dataset['train']['y'], n_bins=params['d_embedding'], regression=True, tree_kwargs=tree_kwargs)\n",
    "    else:\n",
    "        bins = None\n",
    "            \n",
    "    task_type = dataset_info['task_type']\n",
    "    loss_fn = None\n",
    "    \n",
    "    if task_type == 'binclass':\n",
    "        loss_fn = F.binary_cross_entropy_with_logits\n",
    "    elif task_type == 'multiclass':\n",
    "        loss_fn = F.cross_entropy\n",
    "    else:\n",
    "        loss_fn =  F.mse_loss\n",
    "        \n",
    "    return layer_widths, backbone, bins, loss_fn\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функцию, запускающую модель, дополним созданием определеннного ``optimizer``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:31.399215Z",
     "iopub.status.busy": "2025-04-09T13:08:31.398906Z",
     "iopub.status.idle": "2025-04-09T13:08:31.403544Z",
     "shell.execute_reply": "2025-04-09T13:08:31.402579Z",
     "shell.execute_reply.started": "2025-04-09T13:08:31.399189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_optimizer(optim_name, model_params, optuna_params):\n",
    "    optim_class = OPTIMIZERS[optim_name]\n",
    "    optim_kwargs = {'lr' : optuna_params['lr']}\n",
    "    if optim_name != 'muon':\n",
    "        optim_kwargs['weight_decay'] = optuna_params['weight_decay']\n",
    "    return optim_class(model_params, **optim_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:33.669936Z",
     "iopub.status.busy": "2025-04-09T13:08:33.669592Z",
     "iopub.status.idle": "2025-04-09T13:08:33.680112Z",
     "shell.execute_reply": "2025-04-09T13:08:33.679115Z",
     "shell.execute_reply.started": "2025-04-09T13:08:33.669907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_single_model(pkl_path, model_name, emb_name, optim_name, dataset, num_epochs):\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dataset_info = dataset['info']\n",
    "    \n",
    "    optuna_params = read_optuna_params(dataset_info['name'], model_name, emb_name)\n",
    "\n",
    "    num_classes = 1\n",
    "    if dataset_info['task_type'] == 'multiclass':\n",
    "        num_classes = dataset_info['n_classes']\n",
    "    num_cont_cols = dataset['train']['X_num'].shape[1]\n",
    "    \n",
    "    num_params = []\n",
    "    training_time_per_epoch = []\n",
    "\n",
    "    # сохранили КЛАСС этого оптимайзера\n",
    "    def objective(trial):\n",
    "        #возьмем гипперпараметры из оптуны\n",
    "        params = suggest_params(trial, \n",
    "                                optuna_params=optuna_params, \n",
    "                                model_name=model_name, \n",
    "                                emb_name=emb_name, \n",
    "                                optim_name=optim_name)\n",
    "        \n",
    "        # создаем модель и оптимайзер\n",
    "        _, backbone, bins, loss_fn = model_init_preparation(\n",
    "            params=params,\n",
    "            dataset=dataset,\n",
    "            num_classes=num_classes,\n",
    "            model_name=model_name,\n",
    "            emb_name=emb_name\n",
    "        )\n",
    "        model = ModelWithEmbedding(\n",
    "            n_cont_features=num_cont_cols,  # Количество числовых признаков\n",
    "            d_embedding=params['d_embedding'],    # Размерность эмбеддингов\n",
    "            emb_name=emb_name,                # Тип используемого эмбеддинга\n",
    "            backbone_model=backbone,                # Базовая архитектура модели\n",
    "            bins=bins,                    # Параметры бининга для числовых признаков\n",
    "            sigma=params['sigma']          # Параметр sigma для Gaussian слоев\n",
    "        )\n",
    "        model.to(device) \n",
    "        optimizer = get_optimizer(optim_name, model.parameters(), params)\n",
    "        # optimizer = optim_class(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "        \n",
    "        # обучаем модель при данных параметрах\n",
    "        epoch_training_time = train(\n",
    "            epochs=num_epochs,\n",
    "            model=model,\n",
    "            model_emb_name=f'{model_name}_{emb_name}',\n",
    "            device=device,                               \n",
    "            dataset=dataset,                    \n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer, \n",
    "            optimizer_name=optim_name\n",
    "        )\n",
    "        training_time_per_epoch.append(epoch_training_time)\n",
    "        num_params.append(utils.count_parameters(model))\n",
    "\n",
    "        val_loss, val_accuracy, _ = validate(model, device, dataset, loss_fn)\n",
    "        \n",
    "        return (val_loss if dataset_info['task_type'] == 'regression' else val_accuracy)\n",
    "\n",
    "    direction = ('minimize' if dataset_info['task_type'] == 'regression' else 'maximize')\n",
    "    \n",
    "    study = optuna.create_study(direction=direction)\n",
    "    study.optimize(objective, n_trials=70)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    layers, backbone, bins, loss_fn = model_init_preparation(\n",
    "        params=best_params,\n",
    "        dataset=dataset,\n",
    "        num_classes=num_classes,\n",
    "        model_name=model_name,\n",
    "        emb_name=emb_name\n",
    "    )\n",
    "    d_embedding = (best_params['d_embedding'] if emb_name != 'none' else 1)\n",
    "    sigma = (best_params['sigma'] if emb_name == 'periodic' else None)\n",
    "    # lr = best_params['lr']\n",
    "    # weight_decay = best_params['weight_decay']\n",
    "    \n",
    "    test_accuracies = []\n",
    "    test_losses = []\n",
    "    test_times = []\n",
    "    for s in range(10):\n",
    "        utils.seed_everything(s)\n",
    "        model = ModelWithEmbedding(num_cont_cols, d_embedding, emb_name, backbone_model=backbone, bins=bins, sigma=sigma)\n",
    "        model.to(device)   \n",
    "        optimizer = get_optimizer(optim_name, model.parameters(), best_params)\n",
    "       # optimizer = optim_class(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        train(num_epochs, model, f'{model_name}_{emb_name}', device, dataset, loss_fn, optimizer, optim_name)\n",
    "        test_loss, test_accuracy, test_time = validate(model, device, dataset, loss_fn, part='test')\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        test_losses.append(test_loss)\n",
    "        test_times.append(test_time)\n",
    "        \n",
    "    utils.write_results(pkl_path, model_name, emb_name, optim_name, \n",
    "                        layers, num_epochs, num_params, best_params, \n",
    "                        test_accuracies, test_losses, training_time_per_epoch, test_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:37.593911Z",
     "iopub.status.busy": "2025-04-09T13:08:37.593537Z",
     "iopub.status.idle": "2025-04-09T13:08:37.598504Z",
     "shell.execute_reply": "2025-04-09T13:08:37.597551Z",
     "shell.execute_reply.started": "2025-04-09T13:08:37.593881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def run_single_dataset(dataset_name, optim_names, emb_names, model_names, num_epochs):\n",
    "    # dataset_type = dataset_info['type']\n",
    "    dataset = utils.load_dataset(dataset_name)\n",
    "    pkl_path = f'{dataset_name}.pkl'\n",
    "    for model_name in model_names: # можно оставить только kan, тогда model_names = ['kan']\n",
    "        for optim_name in optim_names:\n",
    "            for emb_name in emb_names:\n",
    "                run_single_model(pkl_path, model_name, emb_name, optim_name, dataset, num_epochs)\n",
    "                clear_output(wait=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:08:39.268246Z",
     "iopub.status.busy": "2025-04-09T13:08:39.267934Z",
     "iopub.status.idle": "2025-04-09T13:09:51.691358Z",
     "shell.execute_reply": "2025-04-09T13:09:51.690611Z",
     "shell.execute_reply.started": "2025-04-09T13:08:39.268218Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-09 13:09:36,354] A new study created in memory with name: no-name-31fbccbf-127f-4f31-b3af-e54339368644\n",
      "kan_periodic_mars on eye: 100%|██████████| 10/10 [00:07<00:00,  1.27it/s]\n",
      "[I 2025-04-09 13:09:44,326] Trial 0 finished with value: 0.5428571105003357 and parameters: {'n_layers': 3, 'layer_width': 9, 'lr': 0.00081128612546486, 'weight_decay': 0.0008489616168427463, 'd_embedding': 16, 'sigma': 0.017494562285873944}. Best is trial 0 with value: 0.5428571105003357.\n",
      "kan_periodic_mars on eye: 100%|██████████| 10/10 [00:07<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "optim_names = ['adamw', 'ademamix', 'muon', 'mars']\n",
    "model_names = ['kan']\n",
    "emb_names = ['none', 'periodic']\n",
    "\n",
    "for dataset in ['adult', 'gesture']:\n",
    "    run_single_dataset(dataset, optim_names, emb_names, model_names, 10)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
