{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install delu\n!pip install ucimlrepo\n!pip install gdown\n!pip install rtdl_num_embeddings\n!git clone https://github.com/gbulgakov/testing-kan.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:40:27.757368Z","iopub.execute_input":"2025-04-10T21:40:27.757725Z","iopub.status.idle":"2025-04-10T21:40:44.704611Z","shell.execute_reply.started":"2025-04-10T21:40:27.757693Z","shell.execute_reply":"2025-04-10T21:40:44.703735Z"}},"outputs":[{"name":"stdout","text":"Collecting delu\n  Downloading delu-0.0.26-py3-none-any.whl.metadata (805 bytes)\nRequirement already satisfied: numpy<3,>=1.21 in /usr/local/lib/python3.10/dist-packages (from delu) (1.26.4)\nRequirement already satisfied: torch<3,>=1.9 in /usr/local/lib/python3.10/dist-packages (from delu) (2.5.1+cu121)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.21->delu) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.9->delu) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=1.9->delu) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.9->delu) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.21->delu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.21->delu) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.21->delu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.21->delu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.21->delu) (2024.2.0)\nDownloading delu-0.0.26-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: delu\nSuccessfully installed delu-0.0.26\nCollecting ucimlrepo\n  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.3)\nRequirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2025.1.31)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nDownloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\nInstalling collected packages: ucimlrepo\nSuccessfully installed ucimlrepo-0.0.7\nRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.17.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2025.1.31)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\nCollecting rtdl_num_embeddings\n  Downloading rtdl_num_embeddings-0.0.11-py3-none-any.whl.metadata (882 bytes)\nRequirement already satisfied: torch<3,>=1.12 in /usr/local/lib/python3.10/dist-packages (from rtdl_num_embeddings) (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->rtdl_num_embeddings) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=1.12->rtdl_num_embeddings) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.12->rtdl_num_embeddings) (3.0.2)\nDownloading rtdl_num_embeddings-0.0.11-py3-none-any.whl (13 kB)\nInstalling collected packages: rtdl_num_embeddings\nSuccessfully installed rtdl_num_embeddings-0.0.11\nCloning into 'testing-kan'...\nremote: Enumerating objects: 161, done.\u001b[K\nremote: Total 161 (delta 0), reused 0 (delta 0), pack-reused 161 (from 1)\u001b[K\nReceiving objects: 100% (161/161), 77.66 KiB | 3.24 MiB/s, done.\nResolving deltas: 100% (80/80), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport optuna\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ucimlrepo import fetch_ucirepo\nfrom typing import Literal, Optional\nfrom torch import Tensor\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nimport rtdl_num_embeddings\nimport delu\nfrom IPython.display import FileLink\nfrom tqdm import tqdm\n\n\n# наши импорты\nsys.path.append('/kaggle/working/testing-kan/optimizers')\nsys.path.append('/kaggle/working/testing-kan')\nfrom ademamix import AdEMAMix\nfrom mars import MARS\nfrom muon import Muon\nfrom efficient_kan import KAN\nfrom utils import utils\nfrom utils import tg_bot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:40:44.705905Z","iopub.execute_input":"2025-04-10T21:40:44.706134Z","iopub.status.idle":"2025-04-10T21:40:51.272414Z","shell.execute_reply.started":"2025-04-10T21:40:44.706114Z","shell.execute_reply":"2025-04-10T21:40:51.271677Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# !gdown 1xvRa_-OEeG6xNRYE5V5iAfTwyWM1NiLl # otto\n# !gdown 1tYyhbHdYs_8I9jvXznMoeUAfBzwitaax # house\n# !gdown 1hy1dOAL2SE-XZSuMcjLcVgml2CoYkF9q # higgs-small\n!gdown 1hr076cK9QFxH6YZRg5V4av-H7IAve59r # gesture\n# !gdown 1ZNScy5fgqtgudT6MZ4EjLt1nwdqirtmX # fb-comments\n!gdown 1s0w7gnhiwBCkF49Wdi_cUDpUtXlz2_6q # eye\n!gdown 1T04iP04UGVo95Om84ww1Ed8AFNziOaeY # covtype\n!gdown 1GOkNlinj4zHVSNKbqjN1rR4cvsAf2IgR # churn\n!gdown 11B-l4EasJkclK_Q-RBcxYfGJLSvz-v5c # california\n!gdown 1p8uqDPMfRlFIc69m7iikS6wGkA6JGj1H # adult","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:40:51.274134Z","iopub.execute_input":"2025-04-10T21:40:51.274691Z","iopub.status.idle":"2025-04-10T21:43:04.688000Z","shell.execute_reply.started":"2025-04-10T21:40:51.274636Z","shell.execute_reply":"2025-04-10T21:43:04.686921Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1hr076cK9QFxH6YZRg5V4av-H7IAve59r\nTo: /kaggle/working/gesture.zip\n100%|███████████████████████████████████████| 1.23M/1.23M [00:00<00:00, 101MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1s0w7gnhiwBCkF49Wdi_cUDpUtXlz2_6q\nTo: /kaggle/working/eye.zip\n100%|█████████████████████████████████████████| 534k/534k [00:00<00:00, 104MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1T04iP04UGVo95Om84ww1Ed8AFNziOaeY\nFrom (redirected): https://drive.google.com/uc?id=1T04iP04UGVo95Om84ww1Ed8AFNziOaeY&confirm=t&uuid=b2ab3473-7004-49b2-a912-dee0b994f4df\nTo: /kaggle/working/covtype.zip\n100%|██████████████████████████████████████| 17.2M/17.2M [00:00<00:00, 60.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1GOkNlinj4zHVSNKbqjN1rR4cvsAf2IgR\nTo: /kaggle/working/churn.zip\n100%|█████████████████████████████████████████| 453k/453k [00:00<00:00, 112MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=11B-l4EasJkclK_Q-RBcxYfGJLSvz-v5c\nTo: /kaggle/working/california.zip\n100%|█████████████████████████████████████████| 591k/591k [00:00<00:00, 112MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1p8uqDPMfRlFIc69m7iikS6wGkA6JGj1H\nFrom (redirected): https://drive.google.com/uc?id=1p8uqDPMfRlFIc69m7iikS6wGkA6JGj1H&confirm=t&uuid=415d2ed3-148a-456c-a9a1-a1b9c9e4a48e\nTo: /kaggle/working/adult.zip\n100%|██████████████████████████████████████| 1.12M/1.12M [00:00<00:00, 79.7MB/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Я вынес в файл ``utils`` функции ``count_parameters``,``load_dataset``, ``seed_everything``,  ``write_results``. \nМодели не меняем.","metadata":{}},{"cell_type":"code","source":"DATASETS = ['adult', 'california', 'churn', 'covtype', 'eye', 'fb-comments',\n           'gesture', 'higgs-small', 'house', 'microsoft', 'otto', 'santander']\n\n\nBATCH_SIZES = {'gesture' : 128, 'churn' : 128, 'california' : 256, 'house' : 256, 'adult' : 256, 'otto' : 512, \n               'higgs-small' : 512, 'fb-comments' : 512, 'santander' : 1024, 'covtype' : 1024, 'microsoft' : 1024, 'eye': 128}\n\nREGRESSION = ['house', 'fb-comments', 'microsoft', 'california']\nMULTICLASS = ['covtype', 'eye', 'gesture', 'otto']\nBINCLASS = ['adult', 'churn', 'higgs-small', 'santander']\n\nOPTIMIZERS = {'adamw' : torch.optim.AdamW,\n              'mars' : MARS,\n              'ademamix' : AdEMAMix,\n              'muon' : Muon}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:43:04.689678Z","iopub.execute_input":"2025-04-10T21:43:04.689976Z","iopub.status.idle":"2025-04-10T21:43:04.695819Z","shell.execute_reply.started":"2025-04-10T21:43:04.689952Z","shell.execute_reply":"2025-04-10T21:43:04.694924Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Модели не меняем.","metadata":{}},{"cell_type":"code","source":"class MLP(nn.Sequential):\n    def __init__(self, layers, dropout):\n        super(MLP, self).__init__()\n        \n        total_layers = []\n        for n_in, n_out in zip(layers[:-2], layers[1:-1]):\n            total_layers.append(nn.Linear(n_in, n_out))\n            total_layers.append(nn.SiLU(inplace=False))\n            total_layers.append(nn.Dropout(dropout, inplace=False))\n        total_layers.append(nn.Linear(layers[-2], layers[-1])) # выходной слой\n\n        self.classifier = nn.Sequential(*total_layers)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:43:04.696631Z","iopub.execute_input":"2025-04-10T21:43:04.696924Z","iopub.status.idle":"2025-04-10T21:43:04.717225Z","shell.execute_reply.started":"2025-04-10T21:43:04.696904Z","shell.execute_reply":"2025-04-10T21:43:04.716482Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class ModelWithEmbedding(nn.Module):\n    def __init__(\n        self,\n        n_cont_features,\n        d_embedding,\n        emb_name,\n        backbone_model,\n        bins, sigma=None # словарь всех необязательных параметров, например sigma, bins\n    ) -> None:\n        super().__init__()\n        self.d_embedding = d_embedding\n        self.emb_name = emb_name\n        \n        if emb_name == 'periodic':\n            self.cont_embeddings = rtdl_num_embeddings.PeriodicEmbeddings(\n                n_cont_features, d_embedding, frequency_init_scale=sigma, lite=True\n            )\n            \n        if emb_name == 'piecewiselinearq' or emb_name == 'piecewiselineart':\n            self.cont_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(\n                d_embedding=d_embedding, activation=False, version='B', bins=bins\n            )\n\n        self.backbone = backbone_model\n    \n    def forward(\n        self,\n        x_num : Tensor,\n        x_cat : Optional[Tensor] = None\n    ) -> Tensor:\n        x = []\n        # Step 1. Embed the continuous features.\n        # Flattening is needed for MLP-like models.\n        if self.emb_name != 'none':\n              x_num = self.cont_embeddings(x_num)\n        x.append(x_num.flatten(1))\n        \n        #categorical features do not need embeddings\n        if x_cat is not None:\n            x.append(x_cat.flatten(1))\n        \n        x = torch.column_stack(x)\n        return self.backbone(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:43:04.718012Z","iopub.execute_input":"2025-04-10T21:43:04.718197Z","iopub.status.idle":"2025-04-10T21:43:04.735166Z","shell.execute_reply.started":"2025-04-10T21:43:04.718180Z","shell.execute_reply":"2025-04-10T21:43:04.734248Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"В ``train`` теперь передаем целиком ``optimizer``.  ","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom torch.optim import AdamW, Adam\nfrom torch.optim.lr_scheduler import ExponentialLR, StepLR, CosineAnnealingLR\nfrom torch.nn import MSELoss\nimport torch.nn as nn\nimport time\n\n\ndef apply_model(batch: dict[str, Tensor], model) -> Tensor:\n    return model(batch['X_num'], batch.get('X_cat')).squeeze(-1)\n\n\ndef train(\n    epochs, model, model_emb_name,\n    device, dataset, loss_fn,\n    optimizer,\n    optimizer_name=None\n):\n    # scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n    model.to(device)\n    dataset_name = dataset['info']['id'].split('--')[0]\n    task_type = dataset['info']['task_type']\n    batch_size = BATCH_SIZES[dataset_name]\n\n    times = []\n    for epoch in tqdm(range(epochs), desc = f'{model_emb_name}_{optimizer_name} on {dataset_name}'):\n        start_time = time.time()\n        model.train()\n\n        for data in delu.iter_batches(dataset['train'], shuffle=True, batch_size=batch_size):\n            for key, tensor in data.items():\n                data[key] = tensor.to(device)\n            optimizer.zero_grad()\n            output = apply_model(data, model)\n            if task_type == 'multiclass':\n                data['y'] = data['y'].long()\n            loss_value = loss_fn(output, data['y']) # здесь был каст к типу long (добавил обратно, без него не работает)\n            loss_value.backward()\n            optimizer.step()\n\n        # scheduler.step()\n        end_time = time.time()\n        times.append(end_time-start_time)\n\n    # Return the average times of training epochs\n    t = sum(times)/len(times)\n    return t    \n\ndef validate(model, device, dataset, loss_fn, part='val'):\n    model.eval()\n    model.to(device)\n    val_loss = 0.0\n\n    pred = []\n    gt = [] # настоящие таргеты\n\n    dataset_name = dataset['info']['id'].split('--')[0]\n    task_type = dataset['info']['task_type']\n    batch_size = BATCH_SIZES[dataset_name]\n\n    with torch.no_grad():\n        start_time = time.time()\n        for data in delu.iter_batches(dataset[part], shuffle=False, batch_size=batch_size):\n            for key, tensor in data.items():\n                data[key] = tensor.to(device)\n            output = apply_model(data, model)\n            if task_type == 'multiclass':\n                data['y'] = data['y'].long()\n            val_loss += loss_fn(output, data['y']).item()\n            if output.dim() > 1:\n                pred.append(output.argmax(1))\n            else:\n                pred.append(output >= 0.5)\n            gt.append(data['y'])\n        end_time = time.time()\n        val_time = end_time - start_time\n        \n\n    num_batches = dataset[part]['y'].shape[0] // batch_size + 1\n    pred = torch.cat(pred)\n    gt = torch.cat(gt)\n    val_accuracy = (pred == gt).float().mean().item()\n\n    return val_loss / num_batches, val_accuracy, val_time # с нормировкой\n\ndef train_best_model( #возвращает историю train_loss, val_loss по эпохам\n    epochs, model, model_emb_name, device, dataset, loss_fn, optimizer, optimizer_name=None\n):\n    # scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n    model.to(device)\n    dataset_name = dataset['info']['id'].split('--')[0]\n    task_type = dataset['info']['task_type']\n    batch_size = BATCH_SIZES[dataset_name]\n\n    times = []\n    train_losses = []\n    val_losses = []\n    for epoch in tqdm(range(epochs), desc = f'{model_emb_name}_{optimizer_name} on {dataset_name}'):\n        epoch_train_loss = 0\n        # epoch_val_loss = 0\n        start_time = time.time()\n        model.train()\n        for data in delu.iter_batches(dataset['train'], shuffle=True, batch_size=batch_size):\n            for key, tensor in data.items():\n                data[key] = tensor.to(device)\n            optimizer.zero_grad()\n            output = apply_model(data, model)\n            if task_type == 'multiclass':\n                data['y'] = data['y'].long()\n            loss_value = loss_fn(output, data['y'])\n            epoch_train_loss += loss_value.item()\n            loss_value.backward()\n            optimizer.step()\n\n        # scheduler.step()\n        end_time = time.time()\n        times.append(end_time-start_time)\n        \n        epoch_val_loss, val_accuracy, val_time = validate(model, device, dataset, loss_fn, 'val')\n\n        num_batches = dataset['train']['y'].shape[0] // batch_size + 1\n        train_losses.append(epoch_train_loss / num_batches)\n        val_losses.append(epoch_val_loss)\n    return train_losses, val_losses\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:43:04.736058Z","iopub.execute_input":"2025-04-10T21:43:04.736302Z","iopub.status.idle":"2025-04-10T21:43:04.759093Z","shell.execute_reply.started":"2025-04-10T21:43:04.736274Z","shell.execute_reply":"2025-04-10T21:43:04.758097Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Подбор параметров не меняем, тюним архитектуру сеток, ``lr``, ``weight_decay``.","metadata":{}},{"cell_type":"code","source":"def suggest_params(trial, optuna_params, model_name, emb_name, optim_name):\n    #можно добавить что-то/убрать\n    params = {'n_layers': trial.suggest_int('n_layers', 1, optuna_params['max_n_layer']),\n            'layer_width': trial.suggest_int('layer_width', optuna_params['min_layer_width'], optuna_params['max_layer_width'], step=optuna_params['layer_width_step']),\n            'lr' : trial.suggest_float('lr', optuna_params['min_lr'], optuna_params['max_lr'], log=True)}\n    if optim_name != 'muon':\n        params['weight_decay'] = trial.suggest_float('weight_decay', optuna_params['min_weight_decay'], optuna_params['max_weight_decay'], log=True)\n    \n    params['d_embedding'] = (trial.suggest_int('d_embedding', optuna_params['min_d_embedding'], optuna_params['max_d_embedding']) \n                            if emb_name != 'none'\n                            else 0)\n    \n    if model_name == 'mlp':\n        use_dropout = trial.suggest_categorical('use_dropout', [True, False])\n        params['use_dropout'] = use_dropout\n        params['dropout'] = (trial.suggest_float('dropout', 0, 0.5) if use_dropout else 0)\n    params['sigma'] = (trial.suggest_float('sigma', optuna_params['min_sigma'], optuna_params['max_sigma'], log=True) if emb_name == 'periodic' else None) #дисперсия инициализации весов plr\n    return params\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:43:04.761504Z","iopub.execute_input":"2025-04-10T21:43:04.761795Z","iopub.status.idle":"2025-04-10T21:43:04.779420Z","shell.execute_reply.started":"2025-04-10T21:43:04.761774Z","shell.execute_reply":"2025-04-10T21:43:04.778726Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Поменял размеры (ширину) ``KAN`` и огрубил шаг в ширине до 4.","metadata":{}},{"cell_type":"code","source":"def read_optuna_params(dataset_name, model_name, emb_name):\n    #здесь можно добавить различное пространство гиперпараметров для разных датасетов (пока возвращает все то же самое)\n    params = {'max_n_layer' : 4,\n              'min_layer_width' : (1 if model_name == 'mlp' else 1),\n              'max_layer_width' : (1024 if model_name == 'mlp' else 64),\n              'layer_width_step' : (16 if model_name == 'mlp' else 4),\n              'min_lr' : 1e-4,\n              'max_lr': 1e-2,\n              'min_weight_decay' : 5e-4, # для muon это не актуально, но оставим эти константы\n              'max_weight_decay' : 5e-2}\n\n    if emb_name != 'none':\n        params['max_d_embedding'] = 128\n        params['min_d_embedding'] = 2\n    \n    if emb_name == 'periodic':\n        params['min_sigma'] = 0.01\n        params['max_sigma'] = 100\n\n    return params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:43:04.780345Z","iopub.execute_input":"2025-04-10T21:43:04.780604Z","iopub.status.idle":"2025-04-10T21:43:04.792329Z","shell.execute_reply.started":"2025-04-10T21:43:04.780585Z","shell.execute_reply":"2025-04-10T21:43:04.791532Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def model_init_preparation(params, dataset, num_classes, model_name, emb_name):\n    dataset_info = dataset['info']\n    num_cont_cols = dataset['train']['X_num'].shape[1]\n    num_cat_cols = 0\n    if dataset_info['n_cat_features'] > 0:\n        num_cat_cols = dataset['train']['X_cat'].shape[1]\n\n    # создание модели\n    layer_widths = list(range(params['n_layers'] + 2))\n    \n    if emb_name != 'none':\n        layer_widths[0] = num_cont_cols * params['d_embedding'] + num_cat_cols\n    else:\n        layer_widths[0] = num_cont_cols + num_cat_cols\n    layer_widths[1:-1] = [params['layer_width'] for i in range(params['n_layers'])] #скрытые слои\n    layer_widths[-1] = num_classes\n            \n    if model_name == 'kan':\n        backbone = KAN(layer_widths, grid_size=15, batch_norm=True)\n    elif model_name == 'mlp':\n        dropout = (params['dropout'] if params['use_dropout'] else 0)\n        backbone = MLP(layer_widths, dropout)\n    \n    # создание эмбеддингов\n    if emb_name == 'piecewiselinearq':\n        bins = rtdl_num_embeddings.compute_bins(dataset['train']['X_num'], n_bins=params['d_embedding'])\n    elif emb_name == 'piecewiselineart': # это мы  больше не используем\n        tree_kwargs = {'min_samples_leaf': 64, 'min_impurity_decrease': 1e-4} #возможно стоит тюнить\n        bins = rtdl_num_embeddings.compute_bins(X=dataset['train']['X_num'], y=dataset['train']['y'], n_bins=params['d_embedding'], regression=True, tree_kwargs=tree_kwargs)\n    else:\n        bins = None\n            \n    task_type = dataset_info['task_type']\n    loss_fn = None\n    \n    if task_type == 'binclass':\n        loss_fn = F.binary_cross_entropy_with_logits\n    elif task_type == 'multiclass':\n        loss_fn = F.cross_entropy\n    else:\n        loss_fn =  F.mse_loss\n        \n    return layer_widths, backbone, bins, loss_fn\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:43:04.793166Z","iopub.execute_input":"2025-04-10T21:43:04.793435Z","iopub.status.idle":"2025-04-10T21:43:04.806784Z","shell.execute_reply.started":"2025-04-10T21:43:04.793414Z","shell.execute_reply":"2025-04-10T21:43:04.806051Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Функцию, запускающую модель, дополним созданием определеннного ``optimizer``.","metadata":{}},{"cell_type":"code","source":"def get_optimizer(optim_name, model_params, optuna_params):\n    optim_class = OPTIMIZERS[optim_name]\n    optim_kwargs = {'lr' : optuna_params['lr']}\n    if optim_name != 'muon':\n        optim_kwargs['weight_decay'] = optuna_params['weight_decay']\n    return optim_class(model_params, **optim_kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:43:04.807635Z","iopub.execute_input":"2025-04-10T21:43:04.807969Z","iopub.status.idle":"2025-04-10T21:43:04.823324Z","shell.execute_reply.started":"2025-04-10T21:43:04.807939Z","shell.execute_reply":"2025-04-10T21:43:04.822723Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def run_single_model(pkl_path, model_name, emb_name, optim_name, dataset, num_epochs):\n    \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    dataset_info = dataset['info']\n    \n    optuna_params = read_optuna_params(dataset_info['name'], model_name, emb_name)\n\n    num_classes = 1\n    if dataset_info['task_type'] == 'multiclass':\n        num_classes = dataset_info['n_classes']\n    num_cont_cols = dataset['train']['X_num'].shape[1]\n    \n    num_params = []\n    training_time_per_epoch = []\n\n    # сохранили КЛАСС этого оптимайзера\n    def objective(trial):\n        #возьмем гипперпараметры из оптуны\n        params = suggest_params(trial, \n                                optuna_params=optuna_params, \n                                model_name=model_name, \n                                emb_name=emb_name, \n                                optim_name=optim_name)\n        \n        # создаем модель и оптимайзер\n        _, backbone, bins, loss_fn = model_init_preparation(\n            params=params,\n            dataset=dataset,\n            num_classes=num_classes,\n            model_name=model_name,\n            emb_name=emb_name\n        )\n        model = ModelWithEmbedding(\n            n_cont_features=num_cont_cols,  # Количество числовых признаков\n            d_embedding=params['d_embedding'],    # Размерность эмбеддингов\n            emb_name=emb_name,                # Тип используемого эмбеддинга\n            backbone_model=backbone,                # Базовая архитектура модели\n            bins=bins,                    # Параметры бининга для числовых признаков\n            sigma=params['sigma']          # Параметр sigma для Gaussian слоев\n        )\n        model.to(device) \n        optimizer = get_optimizer(optim_name, model.parameters(), params)\n        # optimizer = optim_class(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n        \n        # обучаем модель при данных параметрах\n        epoch_training_time = train(\n            epochs=num_epochs,\n            model=model,\n            model_emb_name=f'{model_name}_{emb_name}',\n            device=device,                               \n            dataset=dataset,                    \n            loss_fn=loss_fn,\n            optimizer=optimizer, \n            optimizer_name=optim_name\n        )\n        training_time_per_epoch.append(epoch_training_time)\n        num_params.append(utils.count_parameters(model))\n\n        val_loss, val_accuracy, _ = validate(model, device, dataset, loss_fn)\n        \n        return (val_loss if dataset_info['task_type'] == 'regression' else val_accuracy)\n\n    direction = ('minimize' if dataset_info['task_type'] == 'regression' else 'maximize')\n    \n    study = optuna.create_study(direction=direction)\n    study.optimize(objective, n_trials=20)\n    \n    best_params = study.best_params\n    layers, backbone, bins, loss_fn = model_init_preparation(\n        params=best_params,\n        dataset=dataset,\n        num_classes=num_classes,\n        model_name=model_name,\n        emb_name=emb_name\n    )\n    d_embedding = (best_params['d_embedding'] if emb_name != 'none' else 1)\n    sigma = (best_params['sigma'] if emb_name == 'periodic' else None)\n    # lr = best_params['lr']\n    # weight_decay = best_params['weight_decay']\n    \n    test_accuracies = []\n    test_losses = []\n    test_times = []\n    train_loss_history = []\n    val_loss_history = []\n    for s in range(10):\n        utils.seed_everything(s)\n        model = ModelWithEmbedding(num_cont_cols, d_embedding, emb_name, backbone_model=backbone, bins=bins, sigma=sigma)\n        model.to(device)   \n        optimizer = get_optimizer(optim_name, model.parameters(), best_params)\n        # optimizer = optim_class(model.parameters(), lr=lr, weight_decay=weight_decay)\n        #обучение модели + запись истории обучения\n        train_losses, val_losses = train_best_model(3 * num_epochs, model, f'{model_name}_{emb_name}', device, dataset, loss_fn, optimizer, optim_name) #чтобы подробнее посмотреть на историю сделал больше эпох\n        train_loss_history.append(train_losses)\n        val_loss_history.append(val_losses)\n\n        #тестовая часть\n        test_loss, test_accuracy, test_time = validate(model, device, dataset, loss_fn, part='test')\n        test_accuracies.append(test_accuracy)\n        test_losses.append(test_loss)\n        test_times.append(test_time)\n        \n    name = dataset_info['name']\n    tg_bot.send_telegram_message(f'✅ {emb_name}-{model_name} with {optim_name} finished on {name}!')\n        \n    utils.write_results(pkl_path, model_name, emb_name, optim_name, \n                        layers, num_epochs, num_params, best_params, \n                        test_accuracies, test_losses, training_time_per_epoch,\n                        test_times, train_loss_history, val_loss_history)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:43:04.824085Z","iopub.execute_input":"2025-04-10T21:43:04.824300Z","iopub.status.idle":"2025-04-10T21:43:04.861008Z","shell.execute_reply.started":"2025-04-10T21:43:04.824281Z","shell.execute_reply":"2025-04-10T21:43:04.860118Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from IPython.display import clear_output\ndef run_single_dataset(dataset_name, optim_names, emb_names, model_names, num_epochs):\n    # dataset_type = dataset_info['type']\n    dataset = utils.load_dataset(dataset_name)\n    pkl_path = f'{dataset_name}.pkl'\n    for model_name in model_names: # можно оставить только kan, тогда model_names = ['kan']\n        for optim_name in optim_names:\n            for emb_name in emb_names:\n                run_single_model(pkl_path, model_name, emb_name, optim_name, dataset, num_epochs)\n                clear_output(wait=True)\n    tg_bot.send_telegram_message(f'✅ finished on {dataset_name}!\"')\n    tg_bot.send_telegram_file(pkl_path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:43:04.861930Z","iopub.execute_input":"2025-04-10T21:43:04.862254Z","iopub.status.idle":"2025-04-10T21:43:04.880944Z","shell.execute_reply.started":"2025-04-10T21:43:04.862224Z","shell.execute_reply":"2025-04-10T21:43:04.880099Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"optim_names = ['adamw']\nmodel_names = ['kan', 'mlp']\nemb_names = ['none', 'periodic']\n\nfor dataset in ['california', 'churn', 'adult', 'gesture', 'eye']:\n    run_single_dataset(dataset, optim_names, emb_names, model_names, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:43:14.187190Z","iopub.execute_input":"2025-04-10T21:43:14.187546Z","iopub.status.idle":"2025-04-10T22:49:28.699119Z","shell.execute_reply.started":"2025-04-10T21:43:14.187521Z","shell.execute_reply":"2025-04-10T22:49:28.698097Z"}},"outputs":[{"name":"stderr","text":"[I 2025-04-10 22:48:14,200] A new study created in memory with name: no-name-55a7fcfe-4d4f-4cc9-b79f-0050fbf99771\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  6.60it/s]\n[I 2025-04-10 22:48:15,735] Trial 0 finished with value: 0.5274285674095154 and parameters: {'n_layers': 4, 'layer_width': 17, 'lr': 0.0004899268748999498, 'weight_decay': 0.010618198059569418, 'd_embedding': 42, 'use_dropout': True, 'dropout': 0.02966210881135134, 'sigma': 0.013337231110380371}. Best is trial 0 with value: 0.5274285674095154.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  7.15it/s]\n[I 2025-04-10 22:48:17,165] Trial 1 finished with value: 0.5514285564422607 and parameters: {'n_layers': 3, 'layer_width': 737, 'lr': 0.0006019477055652399, 'weight_decay': 0.03291205785480345, 'd_embedding': 22, 'use_dropout': False, 'sigma': 0.1736801954907779}. Best is trial 1 with value: 0.5514285564422607.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  7.17it/s]\n[I 2025-04-10 22:48:18,596] Trial 2 finished with value: 0.5022857189178467 and parameters: {'n_layers': 3, 'layer_width': 769, 'lr': 0.003270202852333467, 'weight_decay': 0.036324896742661934, 'd_embedding': 51, 'use_dropout': False, 'sigma': 0.010473288818007359}. Best is trial 1 with value: 0.5514285564422607.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  8.38it/s]\n[I 2025-04-10 22:48:19,807] Trial 3 finished with value: 0.5485714077949524 and parameters: {'n_layers': 1, 'layer_width': 33, 'lr': 0.00031679573525471343, 'weight_decay': 0.0007690231189671446, 'd_embedding': 69, 'use_dropout': True, 'dropout': 0.0470545485078116, 'sigma': 38.162087946907214}. Best is trial 1 with value: 0.5514285564422607.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  7.09it/s]\n[I 2025-04-10 22:48:21,235] Trial 4 finished with value: 0.5702857375144958 and parameters: {'n_layers': 4, 'layer_width': 113, 'lr': 0.000882126657261354, 'weight_decay': 0.0010758574679573581, 'd_embedding': 8, 'use_dropout': False, 'sigma': 5.3806098303536904}. Best is trial 4 with value: 0.5702857375144958.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  6.44it/s]\n[I 2025-04-10 22:48:22,824] Trial 5 finished with value: 0.6331428289413452 and parameters: {'n_layers': 4, 'layer_width': 497, 'lr': 0.007406962106020899, 'weight_decay': 0.0031375751747938726, 'd_embedding': 108, 'use_dropout': False, 'sigma': 0.53309223104582}. Best is trial 5 with value: 0.6331428289413452.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  6.86it/s]\n[I 2025-04-10 22:48:24,307] Trial 6 finished with value: 0.6777142882347107 and parameters: {'n_layers': 4, 'layer_width': 209, 'lr': 0.0015373875021015885, 'weight_decay': 0.01641525578609907, 'd_embedding': 79, 'use_dropout': False, 'sigma': 0.7969989935335046}. Best is trial 6 with value: 0.6777142882347107.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  8.53it/s]\n[I 2025-04-10 22:48:25,506] Trial 7 finished with value: 0.666857123374939 and parameters: {'n_layers': 1, 'layer_width': 369, 'lr': 0.0048011163082324814, 'weight_decay': 0.006071869233904943, 'd_embedding': 117, 'use_dropout': False, 'sigma': 9.298086449721607}. Best is trial 6 with value: 0.6777142882347107.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  6.83it/s]\n[I 2025-04-10 22:48:26,994] Trial 8 finished with value: 0.6154285669326782 and parameters: {'n_layers': 4, 'layer_width': 145, 'lr': 0.0006180075905222206, 'weight_decay': 0.0014467740628692346, 'd_embedding': 97, 'use_dropout': False, 'sigma': 32.72714063531865}. Best is trial 6 with value: 0.6777142882347107.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  8.24it/s]\n[I 2025-04-10 22:48:28,237] Trial 9 finished with value: 0.6862857341766357 and parameters: {'n_layers': 1, 'layer_width': 545, 'lr': 0.0013671282258525682, 'weight_decay': 0.0021853918676613625, 'd_embedding': 117, 'use_dropout': True, 'dropout': 0.037993572866196534, 'sigma': 5.788894609487758}. Best is trial 9 with value: 0.6862857341766357.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  6.44it/s]\n[I 2025-04-10 22:48:29,868] Trial 10 finished with value: 0.5491428375244141 and parameters: {'n_layers': 2, 'layer_width': 961, 'lr': 0.000150560531091626, 'weight_decay': 0.0029151306029515226, 'd_embedding': 127, 'use_dropout': True, 'dropout': 0.38670661576380516, 'sigma': 4.1382163844365305}. Best is trial 9 with value: 0.6862857341766357.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  7.45it/s]\n[I 2025-04-10 22:48:31,261] Trial 11 finished with value: 0.7011428475379944 and parameters: {'n_layers': 2, 'layer_width': 321, 'lr': 0.0020901057427596156, 'weight_decay': 0.011759674442128751, 'd_embedding': 82, 'use_dropout': True, 'dropout': 0.22349273121223762, 'sigma': 0.7538378661954894}. Best is trial 11 with value: 0.7011428475379944.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  7.20it/s]\n[I 2025-04-10 22:48:32,708] Trial 12 finished with value: 0.6645714044570923 and parameters: {'n_layers': 2, 'layer_width': 385, 'lr': 0.0021291929858240064, 'weight_decay': 0.00688358427586546, 'd_embedding': 88, 'use_dropout': True, 'dropout': 0.23275581692683062, 'sigma': 0.1045954938963161}. Best is trial 11 with value: 0.7011428475379944.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  8.45it/s]\n[I 2025-04-10 22:48:33,953] Trial 13 finished with value: 0.6857143044471741 and parameters: {'n_layers': 1, 'layer_width': 625, 'lr': 0.0017779240586484222, 'weight_decay': 0.0021110905627665156, 'd_embedding': 98, 'use_dropout': True, 'dropout': 0.2070041239691013, 'sigma': 2.0195561408186533}. Best is trial 11 with value: 0.7011428475379944.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  7.88it/s]\n[I 2025-04-10 22:48:35,275] Trial 14 finished with value: 0.6851428747177124 and parameters: {'n_layers': 2, 'layer_width': 337, 'lr': 0.003073557091297292, 'weight_decay': 0.015757318168254954, 'd_embedding': 58, 'use_dropout': True, 'dropout': 0.1413936698395017, 'sigma': 0.20962254006513603}. Best is trial 11 with value: 0.7011428475379944.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  8.80it/s]\n[I 2025-04-10 22:48:36,475] Trial 15 finished with value: 0.6674285531044006 and parameters: {'n_layers': 1, 'layer_width': 545, 'lr': 0.0013057245401236778, 'weight_decay': 0.00410791350072303, 'd_embedding': 74, 'use_dropout': True, 'dropout': 0.3905476959041496, 'sigma': 21.57776108228364}. Best is trial 11 with value: 0.7011428475379944.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  7.41it/s]\n[I 2025-04-10 22:48:37,884] Trial 16 finished with value: 0.6394285559654236 and parameters: {'n_layers': 2, 'layer_width': 289, 'lr': 0.008047737914324359, 'weight_decay': 0.0005163096337270116, 'd_embedding': 128, 'use_dropout': True, 'dropout': 0.11534591595962977, 'sigma': 1.5995387403607286}. Best is trial 11 with value: 0.7011428475379944.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  6.86it/s]\n[I 2025-04-10 22:48:39,405] Trial 17 finished with value: 0.5714285373687744 and parameters: {'n_layers': 3, 'layer_width': 497, 'lr': 0.0002638607015324234, 'weight_decay': 0.009594458642076835, 'd_embedding': 111, 'use_dropout': True, 'dropout': 0.48575637200204225, 'sigma': 98.68369335991838}. Best is trial 11 with value: 0.7011428475379944.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  8.09it/s]\n[I 2025-04-10 22:48:40,700] Trial 18 finished with value: 0.5674285888671875 and parameters: {'n_layers': 1, 'layer_width': 673, 'lr': 0.002917095130759893, 'weight_decay': 0.022429180523824952, 'd_embedding': 39, 'use_dropout': True, 'dropout': 0.3075255633846126, 'sigma': 0.03804413991445979}. Best is trial 11 with value: 0.7011428475379944.\nmlp_periodic_adamw on eye: 100%|██████████| 10/10 [00:01<00:00,  6.18it/s]\n[I 2025-04-10 22:48:42,405] Trial 19 finished with value: 0.6599999666213989 and parameters: {'n_layers': 2, 'layer_width': 913, 'lr': 0.0010250632807693014, 'weight_decay': 0.0018467202551609705, 'd_embedding': 88, 'use_dropout': True, 'dropout': 0.13368843558145782, 'sigma': 0.4604292242851257}. Best is trial 11 with value: 0.7011428475379944.\nmlp_periodic_adamw on eye: 100%|██████████| 30/30 [00:04<00:00,  6.95it/s]\nmlp_periodic_adamw on eye: 100%|██████████| 30/30 [00:04<00:00,  6.88it/s]\nmlp_periodic_adamw on eye: 100%|██████████| 30/30 [00:04<00:00,  6.81it/s]\nmlp_periodic_adamw on eye: 100%|██████████| 30/30 [00:04<00:00,  6.72it/s]\nmlp_periodic_adamw on eye: 100%|██████████| 30/30 [00:04<00:00,  6.68it/s]\nmlp_periodic_adamw on eye: 100%|██████████| 30/30 [00:04<00:00,  6.95it/s]\nmlp_periodic_adamw on eye: 100%|██████████| 30/30 [00:04<00:00,  6.70it/s]\nmlp_periodic_adamw on eye: 100%|██████████| 30/30 [00:04<00:00,  6.98it/s]\nmlp_periodic_adamw on eye: 100%|██████████| 30/30 [00:04<00:00,  7.01it/s]\nmlp_periodic_adamw on eye: 100%|██████████| 30/30 [00:04<00:00,  6.93it/s]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}